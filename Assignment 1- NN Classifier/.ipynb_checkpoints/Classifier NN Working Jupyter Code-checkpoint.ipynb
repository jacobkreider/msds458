{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VSAJFp6ItDAf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CacKZxD1tE61"
   },
   "source": [
    "Starting with Dr Maren code as the base. Then going to split it apart below and reshape to give outputs I'm looking for\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SVcXg4xhZEcH"
   },
   "outputs": [],
   "source": [
    "from random import seed\n",
    "import random\n",
    "from random import randint\n",
    "import pandas as pd\n",
    "\n",
    "# We want to use the exp function (e to the x); \n",
    "# it's part of our transfer function definition\n",
    "from math import exp\n",
    "\n",
    "# Biting the bullet and starting to use NumPy for arrays\n",
    "import numpy as np\n",
    "\n",
    "# So we can make a separate list from an initial one\n",
    "import copy\n",
    "\n",
    "# For pretty-printing the arrays\n",
    "np.set_printoptions(precision=3)\n",
    "np.set_printoptions(suppress=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dpkUtOUp_Jbf"
   },
   "outputs": [],
   "source": [
    "# Some \"worker function\" that eist for specific tasks\n",
    "# Compute neuron activation using sigmoid transfer function\n",
    "def computeTransferFnctn(summedNeuronInput, alpha):\n",
    "    activation = 1.0 / (1.0 + exp(-alpha*summedNeuronInput)) \n",
    "    return activation\n",
    "    \n",
    "# Compute derivative of transfer function\n",
    "def computeTransferFnctnDeriv(NeuronOutput, alpha):\n",
    "    return alpha*NeuronOutput*(1.0 -NeuronOutput)     \n",
    "\n",
    "\n",
    "def matrixDotProduct (matrx1,matrx2):\n",
    "    dotProduct = np.dot(matrx1,matrx2)\n",
    "    \n",
    "    return(dotProduct) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-T2SLVrg_Jx9"
   },
   "outputs": [],
   "source": [
    "# Function to obtain the neural network size specifications\n",
    "\n",
    "def obtainNeuralNetworkSizeSpecs (): \n",
    "\n",
    "    numInputNodes = 81\n",
    "    numHiddenNodes = 6\n",
    "    numOutputNodes = 9   \n",
    "    print()\n",
    "    print(\"  The number of nodes at each level are:\")\n",
    "    print(\"    Input: 9x9 (square array)\")\n",
    "    print(\"    Hidden: \", numHiddenNodes)\n",
    "    print(\"    Output: \", numOutputNodes)\n",
    "            \n",
    "# We create a list containing the crucial SIZES for the connection weight arrays                \n",
    "    arraySizeList = (numInputNodes, numHiddenNodes, numOutputNodes)\n",
    "    \n",
    "# We return this list to the calling procedure, 'main'.       \n",
    "    return (arraySizeList)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_3ylsOod_J9S"
   },
   "outputs": [],
   "source": [
    "# Function to initialize a specific connection weight with a randomly-generated \n",
    "# number between 0 & 1\n",
    "\n",
    "def InitializeWeight ():\n",
    "\n",
    "    randomNum = random.random()\n",
    "    weight=1-2*randomNum\n",
    "#    print weight\n",
    "           \n",
    "    return (weight) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kqfSikBc_KHI"
   },
   "outputs": [],
   "source": [
    "# Function to initialize the node-to-node connection weight arrays\n",
    "\n",
    "def initializeWeightArray (weightArraySizeList):\n",
    "    numLowerNodes = weightArraySizeList[0] \n",
    "    numUpperNodes = weightArraySizeList[1] \n",
    "   \n",
    "    weightArray = np.zeros((numUpperNodes,numLowerNodes))    # iniitalize the weight matrix with 0's\n",
    "    for row in range(numUpperNodes):  #  Number of rows in weightMatrix\n",
    "        # For an input-to-hidden weight matrix, the rows correspond to the number of hidden nodes\n",
    "        #    and the columns correspond to the number of input nodes.\n",
    "        #    This creates an HxI matrix, which can be multiplied by the input matrix (expressed as a column)\n",
    "        # Similarly, for a hidden-to-output matrix, the rows correspond to the number of output nodes.\n",
    "        for col in range(numLowerNodes):  # number of columns in matrix 2\n",
    "            weightArray[row,col] = InitializeWeight ()                 \n",
    "         \n",
    "    return (weightArray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VKXQlvwt_KQ2"
   },
   "outputs": [],
   "source": [
    "# Function to initialize the bias weight arrays\n",
    "\n",
    "def initializeBiasWeightArray (numBiasNodes):\n",
    "    biasWeightArray = np.zeros(numBiasNodes)    # iniitalize the weight matrix with 0's\n",
    "    for node in range(numBiasNodes):  #  Number of nodes in bias weight set\n",
    "        biasWeightArray[node] = InitializeWeight ()\n",
    "      \n",
    "    return (biasWeightArray) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "msjrTbl7AgbQ"
   },
   "outputs": [],
   "source": [
    "# Function to return a trainingDataList\n",
    "\n",
    "def obtainSelectedAlphabetTrainingValues (dataSet):\n",
    "    \n",
    "# Note: Nine possible output classes: 0 .. 8 trainingDataListXX [4]    \n",
    "    trainingDataListA0 =  (1,[0,0,0,0,1,0,0,0,0, 0,0,0,1,0,1,0,0,0, 0,0,1,0,0,0,1,0,0\n",
    "                              , 0,1,0,0,0,0,0,1,0, 1,1,1,1,1,1,1,1,1, 1,0,0,0,0,0,0,0,1\n",
    "                              , 1,0,0,0,0,0,0,0,1, 1,0,0,0,0,0,0,0,1, 1,0,0,0,0,0,0,0,1]\n",
    "                           ,1,'A',0,'A') \n",
    "    trainingDataListB0 =  (2,[1,1,1,1,1,1,1,1,0, 1,0,0,0,0,0,0,0,1, 1,0,0,0,0,0,0,0,1\n",
    "                              , 1,0,0,0,0,0,0,1,0, 1,1,1,1,1,1,1,0,0, 1,0,0,0,0,0,0,1,0\n",
    "                              , 1,0,0,0,0,0,0,0,1, 1,0,0,0,0,0,0,0,1, 1,1,1,1,1,1,1,1,0]\n",
    "                           ,2,'B',1,'B') \n",
    "    trainingDataListC0 =  (3,[1,1,1,1,1,1,1,1,1, 1,0,0,0,0,0,0,0,0, 1,0,0,0,0,0,0,0,0\n",
    "                              , 1,0,0,0,0,0,0,0,0, 1,0,0,0,0,0,0,0,0, 1,0,0,0,0,0,0,0,0\n",
    "                              , 1,0,0,0,0,0,0,0,0, 1,0,0,0,0,0,0,0,0, 1,1,1,1,1,1,1,1,1]\n",
    "                           ,3,'C',2,'C') \n",
    "    trainingDataListD0 =  (4,[1,1,1,1,1,1,1,1,0, 1,0,0,0,0,0,0,1,1, 1,0,0,0,0,0,0,0,1\n",
    "                              , 1,0,0,0,0,0,0,0,1, 1,0,0,0,0,0,0,0,1, 1,0,0,0,0,0,0,0\n",
    "                              ,1, 1,0,0,0,0,0,0,0,1, 1,0,0,0,0,0,0,1,1, 1,1,1,1,1,1,1\n",
    "                              ,1,0],4,'D',3,'O') \n",
    "    trainingDataListE0 =  (5,[1,1,1,1,1,1,1,1,1, 1,0,0,0,0,0,0,0,0, 1,0,0,0,0,0,0,0,0\n",
    "                              , 1,0,0,0,0,0,0,0,0, 1,1,1,1,1,1,1,1,1, 1,0,0,0,0,0,0,0\n",
    "                              ,0, 1,0,0,0,0,0,0,0,0, 1,0,0,0,0,0,0,0,0, 1,1,1,1,1,1,1\n",
    "                              ,1,1],5,'E',4,'E') \n",
    "    trainingDataListF0 =  (6,[1,1,1,1,1,1,1,1,1, 1,0,0,0,0,0,0,0,0, 1,0,0,0,0,0,0,0,0\n",
    "                              , 1,0,0,0,0,0,0,0,0, 1,1,1,1,1,1,1,1,1, 1,0,0,0,0,0,0,0\n",
    "                              ,0, 1,0,0,0,0,0,0,0,0, 1,0,0,0,0,0,0,0,0, 1,0,0,0,0,0,0\n",
    "                              ,0,0],6,'F',4,'E') \n",
    "    trainingDataListG0 =  (7,[1,1,1,1,1,1,1,1,1, 1,0,0,0,0,0,0,0,0, 1,0,0,0,0,0,0,0,0\n",
    "                              , 1,0,0,0,0,0,0,0,0, 1,0,0,0,0,0,0,0,0, 1,0,0,0,0,0,1,1\n",
    "                              ,1, 1,0,0,0,0,0,0,0,1, 1,0,0,0,0,0,0,0,1, 1,1,1,1,1,1,1\n",
    "                              ,1,1],7,'G',1,'C')\n",
    "    trainingDataListH0 =  (8,[1,0,0,0,0,0,0,0,1, 1,0,0,0,0,0,0,0,1, 1,0,0,0,0,0,0,0,1\n",
    "                              , 1,0,0,0,0,0,0,0,1, 1,1,1,1,1,1,1,1,1, 1,0,0,0,0,0,0,0\n",
    "                              ,1, 1,0,0,0,0,0,0,0,1, 1,0,0,0,0,0,0,0,1, 1,0,0,0,0,0,0\n",
    "                              ,0,1],8,'H',0,'A') \n",
    "    trainingDataListI0 =  (9,[0,0,1,1,1,1,1,0,0, 0,0,0,0,1,0,0,0,0, 0,0,0,0,1,0,0,0,0\n",
    "                              , 0,0,0,0,1,0,0,0,0, 0,0,0,0,1,0,0,0,0, 0,0,0,0,1,0,0,0\n",
    "                              ,0, 0,0,0,0,1,0,0,0,0, 0,0,0,0,1,0,0,0,0, 0,0,1,1,1,1,1\n",
    "                              ,0,0],9,'I',5,'I') \n",
    "    trainingDataListJ0 = (10,[0,0,0,0,0,0,0,1,0, 0,0,0,0,0,0,0,1,0, 0,0,0,0,0,0,0,1,0\n",
    "                              , 0,0,0,0,0,0,0,1,0, 0,0,0,0,0,0,0,1,0, 0,1,0,0,0,0,0,1\n",
    "                              ,0, 0,1,0,0,0,0,0,1,0, 0,0,1,0,0,0,1,0,0, 0,0,0,1,1,1,0\n",
    "                              ,0,0],10,'J',5,'I') \n",
    "    trainingDataListK0 = (11,[1,0,0,0,0,0,1,0,0, 1,0,0,0,0,1,0,0,0, 1,0,0,0,1,0,0,0,0\n",
    "                              , 1,0,0,1,0,0,0,0,0, 1,1,1,0,0,0,0,0,0, 1,0,0,1,0,0,0,0\n",
    "                              ,0, 1,0,0,0,1,0,0,0,0, 1,0,0,0,0,1,0,0,0, 1,0,0,0,0,0,1\n",
    "                              ,0,0],11,'K',6,'K')    \n",
    "    trainingDataListL0 = (12,[1,0,0,0,0,0,0,0,0, 1,0,0,0,0,0,0,0,0, 1,0,0,0,0,0,0,0,0\n",
    "                              , 1,0,0,0,0,0,0,0,0, 1,0,0,0,0,0,0,0,0, 1,0,0,0,0,0,0,0\n",
    "                              ,0, 1,0,0,0,0,0,0,0,0, 1,0,0,0,0,0,0,0,0, 1,1,1,1,1,1,1\n",
    "                              ,1,1],12,'L',7,'L') \n",
    "    trainingDataListM0 = (13,[1,0,0,0,0,0,0,0,1, 1,1,0,0,0,0,0,1,1, 1,1,0,0,0,0,0,1,1\n",
    "                              , 1,0,1,0,0,0,1,0,1, 1,0,1,0,0,0,1,0,1, 1,0,0,1,0,1,0,0\n",
    "                              ,1, 1,0,0,1,0,1,0,0,1, 1,1,0,0,1,0,0,0,1, 1,0,0,0,1,0,0\n",
    "                              ,0,1],13,'M',8,'M')            \n",
    "    trainingDataListN0 = (14,[1,0,0,0,0,0,0,0,1, 1,1,0,0,0,0,0,0,1, 1,0,1,0,0,0,0,0,1\n",
    "                              , 1,0,0,1,0,0,0,0,1, 1,0,0,0,1,0,0,0,1, 1,0,0,0,0,1,0,0\n",
    "                              ,1, 1,0,0,0,0,0,1,0,1, 1,0,0,0,0,0,0,1,1, 1,0,0,0,0,0,0\n",
    "                              ,0,1],14,'N',8,'M') \n",
    "    trainingDataListO0 = (15,[0,1,1,1,1,1,1,1,0, 1,1,0,0,0,0,0,1,1, 1,0,0,0,0,0,0,0,1\n",
    "                              , 1,0,0,0,0,0,0,0,1, 1,0,0,0,0,0,0,0,1, 1,0,0,0,0,0,0,0\n",
    "                              ,1, 1,0,0,0,0,0,0,0,1, 1,0,0,0,0,0,0,0,1, 0,1,1,1,1,1,1\n",
    "                              ,1,0],15,'O',3,'O') \n",
    "    trainingDataListP0 = (16,[1,1,1,1,1,1,1,1,1, 1,0,0,0,0,0,0,0,1, 1,0,0,0,0,0,0,0,1\n",
    "                              , 1,0,0,0,0,0,0,0,1, 1,1,1,1,1,1,1,1,1, 1,0,0,0,0,0,0,0\n",
    "                              ,0, 1,0,0,0,0,0,0,0,0, 1,0,0,0,0,0,0,0,0, 1,0,0,0,0,0,0\n",
    "                              ,0,0],16,'P',1, 'B') \n",
    "\n",
    "\n",
    "    if dataSet == 1: trainingDataList = trainingDataListA0\n",
    "    if dataSet == 2: trainingDataList = trainingDataListB0 \n",
    "    if dataSet == 3: trainingDataList = trainingDataListC0\n",
    "    if dataSet == 4: trainingDataList = trainingDataListD0     \n",
    "    if dataSet == 5: trainingDataList = trainingDataListE0\n",
    "    if dataSet == 6: trainingDataList = trainingDataListF0 \n",
    "    if dataSet == 7: trainingDataList = trainingDataListG0 \n",
    "    if dataSet == 8: trainingDataList = trainingDataListH0\n",
    "    if dataSet == 9: trainingDataList = trainingDataListI0\n",
    "    if dataSet == 10: trainingDataList = trainingDataListJ0    \n",
    "\n",
    "    if dataSet == 11: trainingDataList = trainingDataListK0 \n",
    "    if dataSet == 12: trainingDataList = trainingDataListL0\n",
    "    if dataSet == 13: trainingDataList = trainingDataListM0\n",
    "    if dataSet == 14: trainingDataList = trainingDataListN0 \n",
    "    if dataSet == 15: trainingDataList = trainingDataListO0 \n",
    "    if dataSet == 16: trainingDataList = trainingDataListP0  \n",
    "\n",
    "                           \n",
    "    return (trainingDataList)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pc08IcqjE-wf"
   },
   "source": [
    "### Feedforward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4kxVQ5pgeDyT"
   },
   "outputs": [],
   "source": [
    "def ComputeSingleFeedforwardPassFirstStep (alpha, inputDataList, wWeightArray, biasHiddenWeightArray):\n",
    "    \n",
    "# iniitalize the sum of inputs into the hidden array with 0's  \n",
    "    sumIntoHiddenArray = np.zeros(hiddenArrayLength)    \n",
    "    hiddenArray = np.zeros(hiddenArrayLength)   \n",
    "\n",
    "    sumIntoHiddenArray = matrixDotProduct (wWeightArray,inputDataList)\n",
    "    \n",
    "    for node in range(hiddenArrayLength):  #  Number of hidden nodes\n",
    "        hiddenNodeSumInput=sumIntoHiddenArray[node]+biasHiddenWeightArray[node]\n",
    "        hiddenArray[node] = computeTransferFnctn(hiddenNodeSumInput, alpha)\n",
    "\n",
    "#    print ' '\n",
    "#    print 'Back in ComputeSingleFeedforwardPass'\n",
    "#    print 'The activations for the hidden nodes are:'\n",
    "#    print '  Hidden0 = %.4f' % hiddenActivation0, 'Hidden1 = %.4f' % hiddenActivation1\n",
    "\n",
    "                                                                                                    \n",
    "    return (hiddenArray);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xe1MRrWed5V5"
   },
   "outputs": [],
   "source": [
    "def ComputeSingleFeedforwardPassSecondStep (alpha, hiddenArray, vWeightArray, biasOutputWeightArray):\n",
    "    \n",
    "# initialize the sum of inputs into the hidden array with 0's  \n",
    "    sumIntoOutputArray = np.zeros(hiddenArrayLength)    \n",
    "    outputArray = np.zeros(outputArrayLength)   \n",
    "\n",
    "    sumIntoOutputArray = matrixDotProduct (vWeightArray,hiddenArray)\n",
    "    \n",
    "    for node in range(outputArrayLength):  #  Number of hidden nodes\n",
    "        outputNodeSumInput=sumIntoOutputArray[node]+biasOutputWeightArray[node]\n",
    "        outputArray[node] = computeTransferFnctn(outputNodeSumInput, alpha)\n",
    "                                                                                                   \n",
    "    return (outputArray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e_U-w2jCB7qT"
   },
   "source": [
    "For the ComputeOutputsAcrossAllTrainingData in the next section, I actually call this outside of\n",
    "a function in a later section-- I never actually use the function. \n",
    "\n",
    "I found it easier to get the returns I wanted outside of a function call-- the same reason I did away with\n",
    "the 'Main' procedure. Obviously not what you want to do for production work, but for this it made my life a bit easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xyXgCJyzAgq2"
   },
   "outputs": [],
   "source": [
    "# Procedure to compute the output node activations and determine errors across the entire training\n",
    "#  data set, and print results.\n",
    "\n",
    "def ComputeOutputsAcrossAllTrainingData (alpha, numTrainingDataSets, wWeightArray, \n",
    "biasHiddenWeightArray, vWeightArray, biasOutputWeightArray):\n",
    "\n",
    "    selectedTrainingDataSet = 1 \n",
    "                                \n",
    "                              \n",
    "    \n",
    "    allHiddenActivations = [] \n",
    "    while selectedTrainingDataSet < numTrainingDataSets + 1: \n",
    "        print()\n",
    "        print(\" the selected Training Data Set is \", selectedTrainingDataSet)\n",
    "        trainingDataList = obtainSelectedAlphabetTrainingValues (selectedTrainingDataSet)\n",
    " \n",
    "\n",
    "        trainingDataInputList = trainingDataList[1]      \n",
    "        \n",
    "        inputDataList = [] \n",
    "        for node in range(inputArrayLength): \n",
    "            trainingData = trainingDataInputList[node]  \n",
    "            inputDataList.append(trainingData)\n",
    "\n",
    "        letterNum = trainingDataList[2]\n",
    "        letterChar = trainingDataList[3]  \n",
    "        print()\n",
    "        print(\"  Data Set Number\", selectedTrainingDataSet, \" for letter \", letterChar, \" with letter number \", letterNum) \n",
    "\n",
    "        hiddenArray = ComputeSingleFeedforwardPassFirstStep (alpha, inputDataList, wWeightArray, biasHiddenWeightArray)\n",
    "\n",
    "        print()\n",
    "        print(\" The hidden node activations are: \")\n",
    "        print(hiddenArray)\n",
    "        \n",
    "\n",
    "        outputArray = ComputeSingleFeedforwardPassSecondStep (alpha, hiddenArray, vWeightArray, biasOutputWeightArray)\n",
    "    \n",
    "        print()\n",
    "        print(\" The output node activations are: \")\n",
    "        print(outputArray)   \n",
    "\n",
    "        desiredOutputArray = np.zeros(outputArrayLength)    # iniitalize the output array with 0's\n",
    "        desiredClass = trainingDataList[4]                 # identify the desired class\n",
    "        desiredOutputArray[desiredClass] = 1                # set the desired output for that class to 1\n",
    "     \n",
    "        print()\n",
    "        print(\" The desired output array values are: \")\n",
    "        print(desiredOutputArray)  \n",
    "     \n",
    "                        \n",
    "# Determine the error between actual and desired outputs\n",
    "\n",
    "# Initialize the error array\n",
    "        errorArray = np.zeros(outputArrayLength) \n",
    "    \n",
    "        newSSE = 0.0\n",
    "        for node in range(outputArrayLength):  #  Number of nodes in output set (classes)\n",
    "            errorArray[node] = desiredOutputArray[node] - outputArray[node]\n",
    "            newSSE = newSSE + errorArray[node]*errorArray[node]        \n",
    "\n",
    "        print()\n",
    "        print(\" ' The error values are:\")\n",
    "        print(errorArray)   \n",
    "        \n",
    "# Print the Summed Squared Error  \n",
    "        print(\"New SSE = %.6f\" % newSSE) \n",
    "         \n",
    "        selectedTrainingDataSet = selectedTrainingDataSet +1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pcwU9hylCfOo"
   },
   "source": [
    "### Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9j7OXyOwAg16"
   },
   "outputs": [],
   "source": [
    "# Backpropagate weight changes onto the hidden-to-output connection weights\n",
    "\n",
    "def backpropagateOutputToHidden (alpha, eta, arraySizeList, errorArray, outputArray, hiddenArray, vWeightArray):\n",
    "\n",
    "# Unpack array lengths\n",
    "    hiddenArrayLength = arraySizeList [1]\n",
    "    outputArrayLength = arraySizeList [2]\n",
    "\n",
    "    transferFuncDerivArray = np.zeros(outputArrayLength)    \n",
    "      \n",
    "    for node in range(outputArrayLength):  #  Number of hidden nodes\n",
    "        transferFuncDerivArray[node]=computeTransferFnctnDeriv(outputArray[node], alpha)\n",
    "                        \n",
    "    deltaVWtArray = np.zeros((outputArrayLength, hiddenArrayLength))  # initialize an array for the deltas\n",
    "    newVWeightArray = np.zeros((outputArrayLength, hiddenArrayLength)) # initialize an array for the new hidden weights\n",
    "        \n",
    "    for row in range(outputArrayLength):  #  Number of rows in weightMatrix\n",
    "\n",
    "        for col in range(hiddenArrayLength):  # number of columns in weightMatrix\n",
    "            partialSSE_w_V_Wt = -errorArray[row]*transferFuncDerivArray[row]*hiddenArray[col]\n",
    "            deltaVWtArray[row,col] = -eta*partialSSE_w_V_Wt\n",
    "            newVWeightArray[row,col] = vWeightArray[row,col] + deltaVWtArray[row,col]                                                                                        \n",
    "                                                                  \n",
    "                                                                                                                                                                                                            \n",
    "    return (newVWeightArray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3UxImgqeAhAX"
   },
   "outputs": [],
   "source": [
    "# Backpropagate weight changes onto the bias-to-output connection weights\n",
    "\n",
    "def backpropagateBiasOutputWeights (alpha, eta, arraySizeList, errorArray, outputArray, biasOutputWeightArray):\n",
    "\n",
    "#  Unpack the output array length\n",
    "    outputArrayLength = arraySizeList [2]\n",
    "\n",
    "    deltaBiasOutputArray = np.zeros(outputArrayLength)  # initialize an array for the deltas\n",
    "    newBiasOutputWeightArray = np.zeros(outputArrayLength) # initialize an array for the new output bias weights\n",
    "    transferFuncDerivArray = np.zeros(outputArrayLength)    # iniitalize an array for the transfer function\n",
    "      \n",
    "    for node in range(outputArrayLength):  #  Number of hidden nodes\n",
    "        transferFuncDerivArray[node]=computeTransferFnctnDeriv(outputArray[node], alpha)\n",
    " \n",
    "\n",
    "    for node in range(outputArrayLength):  #  Number of nodes in output array (same as number of output bias nodes)    \n",
    "        partialSSE_w_BiasOutput = -errorArray[node]*transferFuncDerivArray[node]\n",
    "        deltaBiasOutputArray[node] = -eta*partialSSE_w_BiasOutput  \n",
    "        newBiasOutputWeightArray[node] =  biasOutputWeightArray[node] + deltaBiasOutputArray[node]           \n",
    "                                                                                                          \n",
    "    return (newBiasOutputWeightArray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xV2U-CR8C8ST"
   },
   "outputs": [],
   "source": [
    "# Backpropagate weight changes onto the input-to-hidden connection weights\n",
    "\n",
    "def backpropagateHiddenToInput (alpha, eta, arraySizeList, errorArray, outputArray, hiddenArray,\n",
    "    inputArray, vWeightArray, wWeightArray, biasHiddenWeightArray, biasOutputWeightArray):\n",
    "\n",
    "# Unpack array lengths\n",
    "    inputArrayLength = arraySizeList [0]\n",
    "    hiddenArrayLength = arraySizeList [1]\n",
    "    outputArrayLength = arraySizeList [2]              \n",
    "                                          \n",
    "\n",
    "    transferFuncDerivHiddenArray = np.zeros(hiddenArrayLength)    \n",
    "    for node in range(hiddenArrayLength):  #  Number of hidden nodes\n",
    "        transferFuncDerivHiddenArray[node]=computeTransferFnctnDeriv(hiddenArray[node], alpha)\n",
    "        \n",
    "    errorTimesTFuncDerivOutputArray = np.zeros(outputArrayLength) # initialize array\n",
    "    transferFuncDerivOutputArray    = np.zeros(outputArrayLength) # initialize array\n",
    "    weightedErrorArray              = np.zeros(hiddenArrayLength) # initialize array\n",
    "      \n",
    "    for outputNode in range(outputArrayLength):  #  Number of output nodes\n",
    "        transferFuncDerivOutputArray[outputNode]=computeTransferFnctnDeriv(outputArray[outputNode], alpha)\n",
    "        errorTimesTFuncDerivOutputArray[outputNode] = errorArray[outputNode]*transferFuncDerivOutputArray[outputNode]\n",
    "        \n",
    "    for hiddenNode in range(hiddenArrayLength):\n",
    "        weightedErrorArray[hiddenNode] = 0\n",
    "        for outputNode in range(outputArrayLength):  #  Number of output nodes    \n",
    "            weightedErrorArray[hiddenNode] = weightedErrorArray[hiddenNode] \\\n",
    "            + vWeightArray[outputNode, hiddenNode]*errorTimesTFuncDerivOutputArray[outputNode]\n",
    "             \n",
    "    deltaWWtArray = np.zeros((hiddenArrayLength, inputArrayLength))  # initialize an array for the deltas\n",
    "    newWWeightArray = np.zeros((hiddenArrayLength, inputArrayLength)) # initialize an array for the new input-to-hidden weights\n",
    "        \n",
    "    for row in range(hiddenArrayLength):  \n",
    "\n",
    "        for col in range(inputArrayLength):  # number of columns in weightMatrix\n",
    "            partialSSE_w_W_Wts = -transferFuncDerivHiddenArray[row]*inputArray[col]*weightedErrorArray[row]\n",
    "            deltaWWtArray[row,col] = -eta*partialSSE_w_W_Wts\n",
    "            newWWeightArray[row,col] = wWeightArray[row,col] + deltaWWtArray[row,col]                                                                                     \n",
    "                                        \n",
    "    return (newWWeightArray)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RFpqA5ptC8YV"
   },
   "outputs": [],
   "source": [
    "# Backpropagate weight changes onto the bias-to-hidden connection weights\n",
    "\n",
    "def backpropagateBiasHiddenWeights (alpha, eta, arraySizeList, errorArray, outputArray, hiddenArray,\n",
    "    inputArray, vWeightArray, wWeightArray, biasHiddenWeightArray, biasOutputWeightArray):\n",
    "# Unpack array lengths\n",
    "    inputArrayLength = arraySizeList [0]\n",
    "    hiddenArrayLength = arraySizeList [1]\n",
    "    outputArrayLength = arraySizeList [2]  \n",
    "               \n",
    "\n",
    "    errorTimesTFuncDerivOutputArray = np.zeros(outputArrayLength)    \n",
    "    transferFuncDerivOutputArray    = np.zeros(outputArrayLength) \n",
    "    weightedErrorArray              = np.zeros(hiddenArrayLength)    \n",
    "\n",
    "    transferFuncDerivHiddenArray = np.zeros(hiddenArrayLength)  \n",
    "    partialSSE_w_BiasHidden      = np.zeros(hiddenArrayLength)  \n",
    "    deltaBiasHiddenArray         = np.zeros(hiddenArrayLength)  \n",
    "    newBiasHiddenWeightArray     = np.zeros(hiddenArrayLength)  \n",
    "          \n",
    "    for node in range(hiddenArrayLength):  #  Number of hidden nodes\n",
    "        transferFuncDerivHiddenArray[node]=computeTransferFnctnDeriv(hiddenArray[node], alpha)      \n",
    "                  \n",
    "    for outputNode in range(outputArrayLength):  #  Number of output nodes\n",
    "        transferFuncDerivOutputArray[outputNode]=computeTransferFnctnDeriv(outputArray[outputNode], alpha) \n",
    "        errorTimesTFuncDerivOutputArray[outputNode] = errorArray[outputNode]*transferFuncDerivOutputArray[outputNode]\n",
    "\n",
    "    for hiddenNode in range(hiddenArrayLength):\n",
    "        weightedErrorArray[hiddenNode] = 0\n",
    "        for outputNode in range(outputArrayLength):  #  Number of output nodes    \n",
    "            weightedErrorArray[hiddenNode] = (weightedErrorArray[hiddenNode]\n",
    "            + vWeightArray[outputNode, hiddenNode]*errorTimesTFuncDerivOutputArray[outputNode])\n",
    "\n",
    "    for hiddenNode in range(hiddenArrayLength):  #  Number of rows in input-to-hidden weightMatrix           \n",
    "        partialSSE_w_BiasHidden[hiddenNode] = -transferFuncDerivHiddenArray[hiddenNode]*weightedErrorArray[hiddenNode]\n",
    "        deltaBiasHiddenArray[hiddenNode] = -eta*partialSSE_w_BiasHidden[hiddenNode]\n",
    "        newBiasHiddenWeightArray[hiddenNode] = biasHiddenWeightArray[hiddenNode] + deltaBiasHiddenArray[hiddenNode]                                                                                                                                                                                                                                                         \n",
    "  \n",
    "                                                                                                                                            \n",
    "    return (newBiasHiddenWeightArray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4P9IgcNAEjzP"
   },
   "source": [
    "### The 'Main' Procedure-- split out into separate code snippets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TcxHx5HZd4Mt"
   },
   "outputs": [],
   "source": [
    "# Define the global variables        \n",
    "global inputArrayLength\n",
    "global hiddenArrayLength\n",
    "global outputArrayLength\n",
    "global gridWidth\n",
    "global gridHeight\n",
    "global eGH # expandedGridHeight, defined in function expandLetterBoundaries \n",
    "global eGW # expandedGridWidth defined in function expandLetterBoundaries \n",
    "global mask1    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 176
    },
    "colab_type": "code",
    "id": "fdqdOIvLiLlz",
    "outputId": "9c6769c0-7858-423a-945f-3fd225920929"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  The number of nodes at each level are:\n",
      "    Input: 9x9 (square array)\n",
      "    Hidden:  6\n",
      "    Output:  9\n",
      "\n",
      " inputArrayLength =  81\n",
      " hiddenArrayLength =  6\n",
      " outputArrayLength =  9\n"
     ]
    }
   ],
   "source": [
    "arraySizeList = list() # empty list\n",
    "\n",
    "# Obtain the actual sizes for each layer of the network       \n",
    "arraySizeList = obtainNeuralNetworkSizeSpecs ()\n",
    "    \n",
    "# Unpack the list; ascribe the various elements of the list to the sizes of different network layers\n",
    "# Note: A word on Python encoding ... the actually length of the array, in each of these three cases, \n",
    "#       will be xArrayLength. For example, the inputArrayLength for the 9x9 pixel array is 81. \n",
    "#       These values are passed to various procedures. They start filling in actual array values,\n",
    "#       where the array values start their count at element 0. However, when filling them in using a\n",
    "#       \"for node in range[limit]\" statement, the \"for\" loop fills from 0 up to limit-1. Thus, the\n",
    "#       original xArrayLength size is preserved.   \n",
    "inputArrayLength = arraySizeList [0] \n",
    "hiddenArrayLength = arraySizeList [1] \n",
    "outputArrayLength = arraySizeList [2] \n",
    "    \n",
    "print()\n",
    "print(\" inputArrayLength = \", inputArrayLength)\n",
    "print(\" hiddenArrayLength = \", hiddenArrayLength)\n",
    "print(\" outputArrayLength = \", outputArrayLength)        \n",
    "\n",
    "\n",
    "# Parameter definitions for backpropagation, to be replaced with user inputs\n",
    "alpha = 1.0\n",
    "eta = 0.5    \n",
    "maxNumIterations = 5000    # temporarily set to 10 for testing\n",
    "epsilon = 0.01\n",
    "iteration = 0\n",
    "SSE = 0.0\n",
    "numTrainingDataSets = 16\n",
    "allHiddenActivations = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "thk64IqjlMXV"
   },
   "outputs": [],
   "source": [
    "# Initialize the weight arrays for two sets of weights; w: input-to-hidden, and v: hidden-to-output\n",
    "####################################################################################################                \n",
    "seed(79)\n",
    "\n",
    "#\n",
    "# The wWeightArray is for Input-to-Hidden\n",
    "# The vWeightArray is for Hidden-to-Output\n",
    "\n",
    "wWeightArraySizeList = (inputArrayLength, hiddenArrayLength)\n",
    "vWeightArraySizeList = (hiddenArrayLength, outputArrayLength)\n",
    "biasHiddenWeightArraySize = hiddenArrayLength\n",
    "biasOutputWeightArraySize = outputArrayLength        \n",
    "\n",
    "# The node-to-node connection weights are stored in a 2-D array\n",
    "\n",
    "wWeightArray = initializeWeightArray (wWeightArraySizeList)\n",
    "  \n",
    "vWeightArray = initializeWeightArray (vWeightArraySizeList)\n",
    "\n",
    "# The bias weights are stored in a 1-D array         \n",
    "biasHiddenWeightArray = initializeBiasWeightArray (biasHiddenWeightArraySize)\n",
    "biasOutputWeightArray = initializeBiasWeightArray (biasOutputWeightArraySize) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4-Jv12Aalrfa"
   },
   "outputs": [],
   "source": [
    "# This is the code from ComputeOutputsAcrossAllTrainingData, modified for\n",
    "# the outputs I am looking for\n",
    "\n",
    "selectedTrainingDataSet = 1 \n",
    "seed(79)\n",
    "                                \n",
    "                              \n",
    "    \n",
    "allHiddenActivationsPreLearn = np.array([])\n",
    "allOutputActivationsPreLearn = np.array([])\n",
    "desiredOutput = np.array([])\n",
    "\n",
    "while selectedTrainingDataSet < numTrainingDataSets + 1: \n",
    "    #print()\n",
    "    #print(\" the selected Training Data Set is \", selectedTrainingDataSet)\n",
    "    trainingDataList = obtainSelectedAlphabetTrainingValues (selectedTrainingDataSet)\n",
    "# Note: the trainingDataList is a list comprising several values:\n",
    "#    - the 0th is the list number \n",
    "#    - the 1st is the actual list of the input training values\n",
    "#    - etc. \n",
    "\n",
    "    trainingDataInputList = trainingDataList[1]      \n",
    "        \n",
    "    inputDataList = [] \n",
    "    for node in range(inputArrayLength): \n",
    "        trainingData = trainingDataInputList[node]  \n",
    "        inputDataList.append(trainingData)\n",
    "\n",
    "    letterNum = trainingDataList[2]\n",
    "    letterChar = trainingDataList[3]  \n",
    "    #print()\n",
    "    #print(\"  Data Set Number\", selectedTrainingDataSet, \" for letter \", letterChar, \" with letter number \", letterNum) \n",
    "\n",
    "    hiddenArray = np.array(ComputeSingleFeedforwardPassFirstStep (alpha, inputDataList, wWeightArray, biasHiddenWeightArray))\n",
    "    allHiddenActivationsPreLearn = np.append(allHiddenActivationsPreLearn, hiddenArray)\n",
    "    \n",
    "    outputArray = ComputeSingleFeedforwardPassSecondStep (alpha, hiddenArray, vWeightArray, biasOutputWeightArray)\n",
    "    allOutputActivationsPreLearn = np.append(allOutputActivationsPreLearn, outputArray)\n",
    "    \n",
    "    desiredOutputArray = np.zeros(outputArrayLength)    # iniitalize the output array with 0's\n",
    "    desiredClass = trainingDataList[4]                 # identify the desired class\n",
    "    desiredOutputArray[desiredClass] = 1\n",
    "    desiredOutput = np.append(desiredOutput, desiredOutputArray)\n",
    "    \n",
    "    \n",
    "    \n",
    "    selectedTrainingDataSet = selectedTrainingDataSet +1\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "id": "_gsGSPyXpafY",
    "outputId": "3645bf90-272f-47fb-c46c-915ba95adc97"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of while loop at iteration  4067\n"
     ]
    }
   ],
   "source": [
    "# Perform backpropagation during each iteration\n",
    "# This code pulled from 'Main' and modified to return the new weights for use\n",
    "# outside of the loop\n",
    "\n",
    "vWeightArrayPost = np.array([])\n",
    "wWeightArrayPost = np.array([])\n",
    "biasHiddenWeightsPost = np.array([])\n",
    "biasOutputWeightsPost = np.array([])\n",
    "\n",
    "while iteration < maxNumIterations: \n",
    "    \n",
    "    # Increment the iteration count\n",
    "    iteration = iteration +1\n",
    "    \n",
    "    vWeightArrayPost = np.array([]) # Re-initializing the new weight arrays \n",
    "    wWeightArrayPost = np.array([]) # so they only contain the final weights\n",
    "    biasHiddenWeightsPost = np.array([])\n",
    "    biasOutputWeightsPost = np.array([])\n",
    "                          \n",
    "    # Re-initialize the training list at the start of each iteration\n",
    "    trainingDataList = (0,0,0,0,0,0,0,0,0\n",
    "                        , 0,0,0,0,0,0,0,0,0, 0,0,0,0,0,0,0,0,0, 0,0,0,0,0,0,0,0,0\n",
    "                        , 0,0,0,0,0,0,0,0,0, 0,0,0,0,0,0,0,0,0, 0,0,0,0,0,0,0,0,0\n",
    "                        , 0,0,0,0,0,0,0,0,0, 0,0,0,0,0,0,0,0,0, 0, ' ')\n",
    "    # Populate the training list with a random data set\n",
    "    dataSet = random.randint(1, numTrainingDataSets)\n",
    "    trainingDataList = obtainSelectedAlphabetTrainingValues(dataSet)\n",
    "    \n",
    "    # Create an input array based on the input training data list\n",
    "    inputDataList = []\n",
    "    inputDataArray = np.zeros(inputArrayLength)\n",
    "    \n",
    "    # Use the items in index 1 as the training inputs\n",
    "    thisTrainingDataList = list()\n",
    "    thisTrainingDataList = trainingDataList[1]\n",
    "    \n",
    "    for node in range(inputArrayLength):\n",
    "      trainingData = thisTrainingDataList[node]\n",
    "      inputDataList.append(trainingData)\n",
    "      inputDataArray[node] = trainingData\n",
    "      \n",
    "    # Create desired output array, from 4th element\n",
    "    desiredOutputArray = np.zeros(outputArrayLength)\n",
    "    desiredClass = trainingDataList[4]\n",
    "    desiredOutputArray[desiredClass] = 1\n",
    "    \n",
    "    # Compute feedforward pass\n",
    "    hiddenArray = ComputeSingleFeedforwardPassFirstStep (alpha, inputDataArray\n",
    "                                                         , wWeightArray\n",
    "                                                         , biasHiddenWeightArray)\n",
    "    outputArray = ComputeSingleFeedforwardPassSecondStep (alpha, hiddenArray\n",
    "                                                          ,vWeightArray\n",
    "                                                          , biasOutputWeightArray)\n",
    "    # Initialize the rror array\n",
    "    errorArray = np.zeros(outputArrayLength)\n",
    "    \n",
    "    # Determine the error and fill the array plus calculate new SSE\n",
    "    newSSE = 0.0\n",
    "    for node in range(outputArrayLength):\n",
    "      errorArray[node] = desiredOutputArray[node] - outputArray[node]\n",
    "      newSSE += errorArray[node]*errorArray[node]\n",
    "      \n",
    "    # Backpropagation\n",
    "    \n",
    "    # Ouput to Hidden backprop\n",
    "    newVWeightArray = backpropagateOutputToHidden (alpha, eta, arraySizeList\n",
    "                                                   , errorArray, outputArray\n",
    "                                                   , hiddenArray, vWeightArray)\n",
    "    \n",
    "    \n",
    "    newBiasOutputWeightArray = backpropagateBiasOutputWeights (alpha, eta\n",
    "                                                               , arraySizeList\n",
    "                                                               , errorArray\n",
    "                                                               , outputArray\n",
    "                                                               , biasOutputWeightArray) \n",
    "    # Hidden to Input backprop\n",
    "    newWWeightArray = backpropagateHiddenToInput (alpha, eta, arraySizeList\n",
    "                                                  , errorArray, outputArray\n",
    "                                                  , hiddenArray, inputDataList\n",
    "                                                  , vWeightArray, wWeightArray\n",
    "                                                  , biasHiddenWeightArray\n",
    "                                                  , biasOutputWeightArray)\n",
    "    newBiasHiddenWeightArray = backpropagateBiasHiddenWeights (alpha, eta\n",
    "                                                               , arraySizeList\n",
    "                                                               , errorArray\n",
    "                                                               , outputArray\n",
    "                                                               , hiddenArray\n",
    "                                                               , inputDataList\n",
    "                                                               , vWeightArray\n",
    "                                                               , wWeightArray\n",
    "                                                               , biasHiddenWeightArray\n",
    "                                                               , biasOutputWeightArray)\n",
    "    \n",
    "    # Update the weight and bias matrices\n",
    "    # Hidden-to-output update\n",
    "    vWeightArray = newVWeightArray[:]\n",
    "    \n",
    "    \n",
    "    biasOutputWeightArray = newBiasOutputWeightArray[:]\n",
    "    \n",
    "    # Input-to-hidden update\n",
    "    wWeightArray = newWWeightArray[:]  \n",
    "    \n",
    "    biasHiddenWeightArray = newBiasHiddenWeightArray[:] \n",
    "    \n",
    "    # Perform a forward pass with the new weights\n",
    "    hiddenArray = ComputeSingleFeedforwardPassFirstStep (alpha, inputDataList\n",
    "                                                         , wWeightArray\n",
    "                                                         , biasHiddenWeightArray)\n",
    "    outputArray = ComputeSingleFeedforwardPassSecondStep (alpha, hiddenArray\n",
    "                                                          , vWeightArray\n",
    "                                                          , biasOutputWeightArray)\n",
    "    \n",
    "    \n",
    "    # Check the new SSE\n",
    "    newSSE = 0.0\n",
    "    for node in range(outputArrayLength):\n",
    "      errorArray[node] - desiredOutputArray[node] - outputArray[node]\n",
    "      newSSE += errorArray[node]*errorArray[node]\n",
    "      \n",
    "    if newSSE < epsilon:\n",
    "      break\n",
    "# Append to our w weight array\n",
    "vWeightArrayPost = vWeightArray\n",
    "wWeightArrayPost = wWeightArray\n",
    "biasOutputWeightsPost = biasOutputWeightArray\n",
    "biasHiddenWeightsPost = biasHiddenWeightArray\n",
    "print(\"Out of while loop at iteration \", iteration)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "  \n",
    "  \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "TAcz4fLNnCuz",
    "outputId": "62e9d5c4-cef0-4293-e08f-2266b6a54b61"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  After training:\n"
     ]
    }
   ],
   "source": [
    "print()\n",
    "print(\"  After training:\")  \n",
    "\n",
    "# This is the code from ComputeOutputsAcrossAllTrainingData, modified for\n",
    "# the outputs I am looking for\n",
    "\n",
    "selectedTrainingDataSet = 1 \n",
    "seed(79)\n",
    "                                \n",
    "                              \n",
    "    \n",
    "allHiddenActivationsPostLearn = np.array([])\n",
    "allOutputActivationsPostLearn = np.array([])\n",
    "desiredOutputPost = np.array([])\n",
    "\n",
    "while selectedTrainingDataSet < numTrainingDataSets + 1: \n",
    "    #print()\n",
    "    #print(\" the selected Training Data Set is \", selectedTrainingDataSet)\n",
    "    trainingDataList = obtainSelectedAlphabetTrainingValues (selectedTrainingDataSet)\n",
    "# Note: the trainingDataList is a list comprising several values:\n",
    "#    - the 0th is the list number \n",
    "#    - the 1st is the actual list of the input training values\n",
    "#    - etc. \n",
    "\n",
    "    trainingDataInputList = trainingDataList[1]      \n",
    "        \n",
    "    inputDataList = [] \n",
    "    for node in range(inputArrayLength): \n",
    "        trainingData = trainingDataInputList[node]  \n",
    "        inputDataList.append(trainingData)\n",
    "\n",
    "    letterNum = trainingDataList[2]\n",
    "    letterChar = trainingDataList[3]  \n",
    "    #print()\n",
    "    #print(\"  Data Set Number\", selectedTrainingDataSet, \" for letter \", letterChar, \" with letter number \", letterNum) \n",
    "\n",
    "    hiddenArray = np.array(ComputeSingleFeedforwardPassFirstStep (alpha, inputDataList, wWeightArrayPost, biasHiddenWeightsPost))\n",
    "    allHiddenActivationsPostLearn = np.append(allHiddenActivationsPostLearn, hiddenArray)\n",
    "    \n",
    "    outputArray = ComputeSingleFeedforwardPassSecondStep (alpha, hiddenArray, vWeightArrayPost, biasOutputWeightsPost)\n",
    "    allOutputActivationsPostLearn = np.append(allOutputActivationsPostLearn, outputArray)\n",
    "    \n",
    "    desiredOutputArray = np.zeros(outputArrayLength)    # iniitalize the output array with 0's\n",
    "    desiredClass = trainingDataList[4]                 # identify the desired class\n",
    "    desiredOutputArray[desiredClass] = 1\n",
    "    desiredOutputPost = np.append(desiredOutputPost, desiredOutputArray)\n",
    "    \n",
    "    \n",
    "    \n",
    "    selectedTrainingDataSet = selectedTrainingDataSet +1\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YlCZZjPxF_Zw"
   },
   "source": [
    "### Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "whjW1r8em_Ft"
   },
   "outputs": [],
   "source": [
    "# Create dataframes for pre-training values of all activations\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "preHidden = pd.DataFrame([allHiddenActivationsPreLearn[0:6]\n",
    "                         ,allHiddenActivationsPreLearn[6:12]\n",
    "                         ,allHiddenActivationsPreLearn[12:18]\n",
    "                         ,allHiddenActivationsPreLearn[18:24]\n",
    "                         ,allHiddenActivationsPreLearn[24:30]\n",
    "                         ,allHiddenActivationsPreLearn[30:36]\n",
    "                         ,allHiddenActivationsPreLearn[36:42]\n",
    "                         ,allHiddenActivationsPreLearn[42:48]\n",
    "                         ,allHiddenActivationsPreLearn[48:54]\n",
    "                         ,allHiddenActivationsPreLearn[54:60]\n",
    "                         ,allHiddenActivationsPreLearn[60:66]\n",
    "                         ,allHiddenActivationsPreLearn[66:72]\n",
    "                         ,allHiddenActivationsPreLearn[72:78]\n",
    "                         ,allHiddenActivationsPreLearn[78:84]\n",
    "                         ,allHiddenActivationsPreLearn[84:90]\n",
    "                         ,allHiddenActivationsPreLearn[90:96]]\n",
    "                         , index = [\"A\", \"B\", \"C\", \"D\"\n",
    "                                   ,\"E\", \"F\", \"G\", \"H\"\n",
    "                                   ,\"I\", \"J\", \"K\", \"L\"\n",
    "                                   ,\"M\", \"N\", \"O\", \"P\"]\n",
    "                        , columns = [\"H0\", \"H1\", \"H2\"\n",
    "                                    , \"H3\", \"H4\", \"H5\"])\n",
    "\n",
    "preOutput = pd.DataFrame([allOutputActivationsPreLearn[0:9]\n",
    "                         ,allOutputActivationsPreLearn[9:18]\n",
    "                         ,allOutputActivationsPreLearn[18:27]\n",
    "                         ,allOutputActivationsPreLearn[27:36]\n",
    "                         ,allOutputActivationsPreLearn[36:45]\n",
    "                         ,allOutputActivationsPreLearn[45:54]\n",
    "                         ,allOutputActivationsPreLearn[54:63]\n",
    "                         ,allOutputActivationsPreLearn[63:72]\n",
    "                         ,allOutputActivationsPreLearn[72:81]\n",
    "                         ,allOutputActivationsPreLearn[81:90]\n",
    "                         ,allOutputActivationsPreLearn[90:99]\n",
    "                         ,allOutputActivationsPreLearn[99:108]\n",
    "                         ,allOutputActivationsPreLearn[108:117]\n",
    "                         ,allOutputActivationsPreLearn[117:126]\n",
    "                         ,allOutputActivationsPreLearn[126:135]\n",
    "                         ,allOutputActivationsPreLearn[135:144]]\n",
    "                         , index = [\"A\", \"B\", \"C\", \"D\"\n",
    "                                   ,\"E\", \"F\", \"G\", \"H\"\n",
    "                                   ,\"I\", \"J\", \"K\", \"L\"\n",
    "                                   ,\"M\", \"N\", \"O\", \"P\"]\n",
    "                        , columns = [\"o0\", \"o1\", \"o2\"\n",
    "                                    , \"o3\", \"o4\", \"o5\"\n",
    "                                    , \"o6\", \"o7\", \"o8\"])\n",
    "\n",
    "desired = pd.DataFrame([desiredOutput[0:9]\n",
    "                         ,desiredOutput[9:18]\n",
    "                         ,desiredOutput[18:27]\n",
    "                         ,desiredOutput[27:36]\n",
    "                         ,desiredOutput[36:45]\n",
    "                         ,desiredOutput[45:54]\n",
    "                         ,desiredOutput[54:63]\n",
    "                         ,desiredOutput[63:72]\n",
    "                         ,desiredOutput[72:81]\n",
    "                         ,desiredOutput[81:90]\n",
    "                         ,desiredOutput[90:99]\n",
    "                         ,desiredOutput[99:108]\n",
    "                         ,desiredOutput[108:117]\n",
    "                         ,desiredOutput[117:126]\n",
    "                         ,desiredOutput[126:135]\n",
    "                         ,desiredOutput[135:144]]\n",
    "                         , index = [\"A\", \"B\", \"C\", \"D\"\n",
    "                                   ,\"E\", \"F\", \"G\", \"H\"\n",
    "                                   ,\"I\", \"J\", \"K\", \"L\"\n",
    "                                   ,\"M\", \"N\", \"O\", \"P\"]\n",
    "                        , columns = [\"o0\", \"o1\", \"o2\"\n",
    "                                    , \"o3\", \"o4\", \"o5\"\n",
    "                                    , \"o6\", \"o7\", \"o8\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sIkARr96pbhW"
   },
   "outputs": [],
   "source": [
    "# Create data frames for post-output values of all activations\n",
    "\n",
    "postHidden = pd.DataFrame([allHiddenActivationsPostLearn[0:6]\n",
    "                         ,allHiddenActivationsPostLearn[6:12]\n",
    "                         ,allHiddenActivationsPostLearn[12:18]\n",
    "                         ,allHiddenActivationsPostLearn[18:24]\n",
    "                         ,allHiddenActivationsPostLearn[24:30]\n",
    "                         ,allHiddenActivationsPostLearn[30:36]\n",
    "                         ,allHiddenActivationsPostLearn[36:42]\n",
    "                         ,allHiddenActivationsPostLearn[42:48]\n",
    "                         ,allHiddenActivationsPostLearn[48:54]\n",
    "                         ,allHiddenActivationsPostLearn[54:60]\n",
    "                         ,allHiddenActivationsPostLearn[60:66]\n",
    "                         ,allHiddenActivationsPostLearn[66:72]\n",
    "                         ,allHiddenActivationsPostLearn[72:78]\n",
    "                         ,allHiddenActivationsPostLearn[78:84]\n",
    "                         ,allHiddenActivationsPostLearn[84:90]\n",
    "                         ,allHiddenActivationsPostLearn[90:96]]\n",
    "                         , index = [\"A\", \"B\", \"C\", \"D\"\n",
    "                                   ,\"E\", \"F\", \"G\", \"H\"\n",
    "                                   ,\"I\", \"J\", \"K\", \"L\"\n",
    "                                   ,\"M\", \"N\", \"O\", \"P\"]\n",
    "                        , columns = [\"H0\", \"H1\", \"H2\"\n",
    "                                    , \"H3\", \"H4\", \"H5\"])\n",
    "\n",
    "postOutput = pd.DataFrame([allOutputActivationsPostLearn[0:9]\n",
    "                         ,allOutputActivationsPostLearn[9:18]\n",
    "                         ,allOutputActivationsPostLearn[18:27]\n",
    "                         ,allOutputActivationsPostLearn[27:36]\n",
    "                         ,allOutputActivationsPostLearn[36:45]\n",
    "                         ,allOutputActivationsPostLearn[45:54]\n",
    "                         ,allOutputActivationsPostLearn[54:63]\n",
    "                         ,allOutputActivationsPostLearn[63:72]\n",
    "                         ,allOutputActivationsPostLearn[72:81]\n",
    "                         ,allOutputActivationsPostLearn[81:90]\n",
    "                         ,allOutputActivationsPostLearn[90:99]\n",
    "                         ,allOutputActivationsPostLearn[99:108]\n",
    "                         ,allOutputActivationsPostLearn[108:117]\n",
    "                         ,allOutputActivationsPostLearn[117:126]\n",
    "                         ,allOutputActivationsPostLearn[126:135]\n",
    "                         ,allOutputActivationsPostLearn[135:144]]\n",
    "                         , index = [\"A\", \"B\", \"C\", \"D\"\n",
    "                                   ,\"E\", \"F\", \"G\", \"H\"\n",
    "                                   ,\"I\", \"J\", \"K\", \"L\"\n",
    "                                   ,\"M\", \"N\", \"O\", \"P\"]\n",
    "                        , columns = [\"o0\", \"o1\", \"o2\"\n",
    "                                    , \"o3\", \"o4\", \"o5\"\n",
    "                                    , \"o6\", \"o7\", \"o8\"])\n",
    "\n",
    "postDesired = pd.DataFrame([desiredOutput[0:9]\n",
    "                         ,desiredOutput[9:18]\n",
    "                         ,desiredOutput[18:27]\n",
    "                         ,desiredOutput[27:36]\n",
    "                         ,desiredOutput[36:45]\n",
    "                         ,desiredOutput[45:54]\n",
    "                         ,desiredOutput[54:63]\n",
    "                         ,desiredOutput[63:72]\n",
    "                         ,desiredOutput[72:81]\n",
    "                         ,desiredOutput[81:90]\n",
    "                         ,desiredOutput[90:99]\n",
    "                         ,desiredOutput[99:108]\n",
    "                         ,desiredOutput[108:117]\n",
    "                         ,desiredOutput[117:126]\n",
    "                         ,desiredOutput[126:135]\n",
    "                         ,desiredOutput[135:144]]\n",
    "                         , index = [\"A\", \"B\", \"C\", \"D\"\n",
    "                                   ,\"E\", \"F\", \"G\", \"H\"\n",
    "                                   ,\"I\", \"J\", \"K\", \"L\"\n",
    "                                   ,\"M\", \"N\", \"O\", \"P\"]\n",
    "                        , columns = [\"o0\", \"o1\", \"o2\"\n",
    "                                    , \"o3\", \"o4\", \"o5\"\n",
    "                                    , \"o6\", \"o7\", \"o8\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1906
    },
    "colab_type": "code",
    "id": "RHfBPIooGC5p",
    "outputId": "e0146f98-623a-4f4a-edf6-be5c1bc35894"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>H0</th>\n",
       "      <th>H1</th>\n",
       "      <th>H2</th>\n",
       "      <th>H3</th>\n",
       "      <th>H4</th>\n",
       "      <th>H5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.434504</td>\n",
       "      <td>-0.488783</td>\n",
       "      <td>0.130245</td>\n",
       "      <td>0.660601</td>\n",
       "      <td>0.308181</td>\n",
       "      <td>0.199522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.749510</td>\n",
       "      <td>0.084882</td>\n",
       "      <td>0.789451</td>\n",
       "      <td>1.588133</td>\n",
       "      <td>-0.308554</td>\n",
       "      <td>0.000462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.139928</td>\n",
       "      <td>0.368412</td>\n",
       "      <td>-0.663431</td>\n",
       "      <td>0.226990</td>\n",
       "      <td>-0.265909</td>\n",
       "      <td>1.373826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.974014</td>\n",
       "      <td>-0.583948</td>\n",
       "      <td>0.741883</td>\n",
       "      <td>-0.267954</td>\n",
       "      <td>-1.537096</td>\n",
       "      <td>1.096118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.042879</td>\n",
       "      <td>0.154414</td>\n",
       "      <td>0.881985</td>\n",
       "      <td>0.613308</td>\n",
       "      <td>0.094439</td>\n",
       "      <td>0.848292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.155759</td>\n",
       "      <td>-0.951931</td>\n",
       "      <td>1.115433</td>\n",
       "      <td>1.081285</td>\n",
       "      <td>-0.874828</td>\n",
       "      <td>0.564274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.505356</td>\n",
       "      <td>0.136741</td>\n",
       "      <td>1.036721</td>\n",
       "      <td>0.070874</td>\n",
       "      <td>-0.928161</td>\n",
       "      <td>0.500638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.213584</td>\n",
       "      <td>0.524270</td>\n",
       "      <td>-0.546613</td>\n",
       "      <td>-0.170270</td>\n",
       "      <td>0.021695</td>\n",
       "      <td>0.721247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-1.173564</td>\n",
       "      <td>0.339249</td>\n",
       "      <td>-0.859683</td>\n",
       "      <td>1.063074</td>\n",
       "      <td>-1.042430</td>\n",
       "      <td>0.022618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.150762</td>\n",
       "      <td>-0.047006</td>\n",
       "      <td>0.808330</td>\n",
       "      <td>0.573355</td>\n",
       "      <td>0.497316</td>\n",
       "      <td>0.215781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.167154</td>\n",
       "      <td>0.006036</td>\n",
       "      <td>0.766078</td>\n",
       "      <td>-0.774527</td>\n",
       "      <td>0.310309</td>\n",
       "      <td>0.717661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.180043</td>\n",
       "      <td>-0.559716</td>\n",
       "      <td>0.037415</td>\n",
       "      <td>-0.232565</td>\n",
       "      <td>0.746163</td>\n",
       "      <td>-0.870061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.694271</td>\n",
       "      <td>-1.243527</td>\n",
       "      <td>-0.630003</td>\n",
       "      <td>-0.708625</td>\n",
       "      <td>-0.653024</td>\n",
       "      <td>-0.121635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.281620</td>\n",
       "      <td>1.233001</td>\n",
       "      <td>-0.594069</td>\n",
       "      <td>-0.821473</td>\n",
       "      <td>0.220906</td>\n",
       "      <td>0.777587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.932143</td>\n",
       "      <td>-0.289536</td>\n",
       "      <td>0.450891</td>\n",
       "      <td>1.203512</td>\n",
       "      <td>0.765258</td>\n",
       "      <td>-0.606068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.235748</td>\n",
       "      <td>-0.415700</td>\n",
       "      <td>-0.997427</td>\n",
       "      <td>-0.277788</td>\n",
       "      <td>-0.040600</td>\n",
       "      <td>-0.751170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.655208</td>\n",
       "      <td>0.715417</td>\n",
       "      <td>-0.119034</td>\n",
       "      <td>-0.446623</td>\n",
       "      <td>1.272396</td>\n",
       "      <td>0.518244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-1.019531</td>\n",
       "      <td>-2.378617</td>\n",
       "      <td>1.802843</td>\n",
       "      <td>-1.772659</td>\n",
       "      <td>-0.316467</td>\n",
       "      <td>0.985079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.749305</td>\n",
       "      <td>0.421106</td>\n",
       "      <td>0.535920</td>\n",
       "      <td>0.519077</td>\n",
       "      <td>0.987311</td>\n",
       "      <td>-0.042310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.941119</td>\n",
       "      <td>0.254164</td>\n",
       "      <td>0.374065</td>\n",
       "      <td>0.227912</td>\n",
       "      <td>0.344252</td>\n",
       "      <td>-0.607863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.093666</td>\n",
       "      <td>-0.316261</td>\n",
       "      <td>-0.135934</td>\n",
       "      <td>-0.614475</td>\n",
       "      <td>0.835061</td>\n",
       "      <td>0.385332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-0.479980</td>\n",
       "      <td>-0.620301</td>\n",
       "      <td>-0.935197</td>\n",
       "      <td>0.365424</td>\n",
       "      <td>-0.801617</td>\n",
       "      <td>-0.795194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-0.152475</td>\n",
       "      <td>-0.015360</td>\n",
       "      <td>-0.282896</td>\n",
       "      <td>0.302966</td>\n",
       "      <td>0.961536</td>\n",
       "      <td>-0.360577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.842410</td>\n",
       "      <td>0.419068</td>\n",
       "      <td>-0.649329</td>\n",
       "      <td>0.291106</td>\n",
       "      <td>0.675934</td>\n",
       "      <td>0.333390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.839375</td>\n",
       "      <td>-0.826415</td>\n",
       "      <td>0.253794</td>\n",
       "      <td>-0.333662</td>\n",
       "      <td>0.600911</td>\n",
       "      <td>1.046372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.854434</td>\n",
       "      <td>-0.452231</td>\n",
       "      <td>0.233038</td>\n",
       "      <td>0.305643</td>\n",
       "      <td>0.111555</td>\n",
       "      <td>0.047781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.557987</td>\n",
       "      <td>-1.248075</td>\n",
       "      <td>1.369917</td>\n",
       "      <td>-0.348699</td>\n",
       "      <td>-1.192376</td>\n",
       "      <td>0.819762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.460134</td>\n",
       "      <td>0.597558</td>\n",
       "      <td>0.349213</td>\n",
       "      <td>0.329658</td>\n",
       "      <td>0.600580</td>\n",
       "      <td>-0.047959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.419364</td>\n",
       "      <td>0.439031</td>\n",
       "      <td>-0.176980</td>\n",
       "      <td>-0.324308</td>\n",
       "      <td>-0.087381</td>\n",
       "      <td>0.182621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.445397</td>\n",
       "      <td>-0.427448</td>\n",
       "      <td>0.516395</td>\n",
       "      <td>-0.742909</td>\n",
       "      <td>-0.161144</td>\n",
       "      <td>-0.442624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>-0.095972</td>\n",
       "      <td>-0.032877</td>\n",
       "      <td>-0.423947</td>\n",
       "      <td>0.972194</td>\n",
       "      <td>-0.488823</td>\n",
       "      <td>-0.143386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0.446892</td>\n",
       "      <td>-1.507813</td>\n",
       "      <td>-0.645050</td>\n",
       "      <td>1.873589</td>\n",
       "      <td>-2.073565</td>\n",
       "      <td>0.937493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>-1.294172</td>\n",
       "      <td>-1.205483</td>\n",
       "      <td>0.678547</td>\n",
       "      <td>-0.750763</td>\n",
       "      <td>-0.416279</td>\n",
       "      <td>0.997814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>-1.151571</td>\n",
       "      <td>0.137427</td>\n",
       "      <td>0.566030</td>\n",
       "      <td>-0.732969</td>\n",
       "      <td>-0.178841</td>\n",
       "      <td>0.746598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>-0.338517</td>\n",
       "      <td>-0.397203</td>\n",
       "      <td>-0.109926</td>\n",
       "      <td>0.316335</td>\n",
       "      <td>-0.958326</td>\n",
       "      <td>-0.578976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0.754814</td>\n",
       "      <td>0.428090</td>\n",
       "      <td>-0.240482</td>\n",
       "      <td>0.250419</td>\n",
       "      <td>-0.162731</td>\n",
       "      <td>0.696982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>-1.079742</td>\n",
       "      <td>-0.963979</td>\n",
       "      <td>-0.062609</td>\n",
       "      <td>0.484420</td>\n",
       "      <td>-0.294141</td>\n",
       "      <td>-0.204385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0.735044</td>\n",
       "      <td>0.062679</td>\n",
       "      <td>0.211430</td>\n",
       "      <td>0.886332</td>\n",
       "      <td>-0.871089</td>\n",
       "      <td>-0.012380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>-0.548946</td>\n",
       "      <td>-0.446550</td>\n",
       "      <td>-0.308555</td>\n",
       "      <td>0.128953</td>\n",
       "      <td>-0.459535</td>\n",
       "      <td>-0.076725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>-0.640384</td>\n",
       "      <td>0.401236</td>\n",
       "      <td>0.863009</td>\n",
       "      <td>0.381935</td>\n",
       "      <td>-0.569968</td>\n",
       "      <td>0.599959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>0.361599</td>\n",
       "      <td>0.946000</td>\n",
       "      <td>0.862303</td>\n",
       "      <td>-0.381916</td>\n",
       "      <td>-0.700446</td>\n",
       "      <td>0.694223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>-0.326819</td>\n",
       "      <td>-0.558281</td>\n",
       "      <td>-0.801178</td>\n",
       "      <td>0.815710</td>\n",
       "      <td>0.113441</td>\n",
       "      <td>0.073539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>0.332621</td>\n",
       "      <td>0.081485</td>\n",
       "      <td>0.158249</td>\n",
       "      <td>-0.020840</td>\n",
       "      <td>0.460466</td>\n",
       "      <td>-0.594142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0.148344</td>\n",
       "      <td>0.333924</td>\n",
       "      <td>0.357134</td>\n",
       "      <td>-0.133556</td>\n",
       "      <td>-0.677442</td>\n",
       "      <td>-0.282914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>0.669950</td>\n",
       "      <td>1.192698</td>\n",
       "      <td>0.742461</td>\n",
       "      <td>-0.333723</td>\n",
       "      <td>-0.921895</td>\n",
       "      <td>0.766686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>-0.479485</td>\n",
       "      <td>-0.486320</td>\n",
       "      <td>0.079525</td>\n",
       "      <td>-0.117038</td>\n",
       "      <td>0.074168</td>\n",
       "      <td>0.173929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>-0.222652</td>\n",
       "      <td>0.799349</td>\n",
       "      <td>0.972326</td>\n",
       "      <td>0.382260</td>\n",
       "      <td>-0.063726</td>\n",
       "      <td>0.198309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>-0.755817</td>\n",
       "      <td>-0.162976</td>\n",
       "      <td>-0.220533</td>\n",
       "      <td>-0.269564</td>\n",
       "      <td>0.682552</td>\n",
       "      <td>0.414198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>-0.281772</td>\n",
       "      <td>0.715849</td>\n",
       "      <td>0.083670</td>\n",
       "      <td>-1.193940</td>\n",
       "      <td>0.196216</td>\n",
       "      <td>0.151308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>-0.132035</td>\n",
       "      <td>0.018131</td>\n",
       "      <td>0.176987</td>\n",
       "      <td>-0.418749</td>\n",
       "      <td>0.239076</td>\n",
       "      <td>0.604415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>-0.526967</td>\n",
       "      <td>-2.054435</td>\n",
       "      <td>-0.138487</td>\n",
       "      <td>-0.640927</td>\n",
       "      <td>0.379745</td>\n",
       "      <td>-0.542073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0.583725</td>\n",
       "      <td>-0.753574</td>\n",
       "      <td>-1.182256</td>\n",
       "      <td>0.039511</td>\n",
       "      <td>0.360337</td>\n",
       "      <td>0.306739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0.621524</td>\n",
       "      <td>0.223446</td>\n",
       "      <td>0.057783</td>\n",
       "      <td>0.350637</td>\n",
       "      <td>0.776465</td>\n",
       "      <td>-0.262974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0.260777</td>\n",
       "      <td>0.689511</td>\n",
       "      <td>-0.858864</td>\n",
       "      <td>-0.105743</td>\n",
       "      <td>0.035360</td>\n",
       "      <td>-0.798797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>0.546500</td>\n",
       "      <td>0.699077</td>\n",
       "      <td>0.147395</td>\n",
       "      <td>-1.473438</td>\n",
       "      <td>-0.533398</td>\n",
       "      <td>-0.379967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>-0.568421</td>\n",
       "      <td>0.023650</td>\n",
       "      <td>0.119810</td>\n",
       "      <td>-0.726894</td>\n",
       "      <td>-0.501216</td>\n",
       "      <td>-0.115364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>1.179620</td>\n",
       "      <td>0.507862</td>\n",
       "      <td>0.160949</td>\n",
       "      <td>-0.840718</td>\n",
       "      <td>-0.673402</td>\n",
       "      <td>0.173014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>-0.024664</td>\n",
       "      <td>-0.032842</td>\n",
       "      <td>-0.873934</td>\n",
       "      <td>-0.118272</td>\n",
       "      <td>0.371869</td>\n",
       "      <td>-1.120477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0.403405</td>\n",
       "      <td>0.725065</td>\n",
       "      <td>-0.820760</td>\n",
       "      <td>0.180681</td>\n",
       "      <td>1.077428</td>\n",
       "      <td>0.046858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.237640</td>\n",
       "      <td>0.456697</td>\n",
       "      <td>-0.651158</td>\n",
       "      <td>-1.286736</td>\n",
       "      <td>-0.163607</td>\n",
       "      <td>0.626159</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>81 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          H0        H1        H2        H3        H4        H5\n",
       "0   0.434504 -0.488783  0.130245  0.660601  0.308181  0.199522\n",
       "1   0.749510  0.084882  0.789451  1.588133 -0.308554  0.000462\n",
       "2   1.139928  0.368412 -0.663431  0.226990 -0.265909  1.373826\n",
       "3   0.974014 -0.583948  0.741883 -0.267954 -1.537096  1.096118\n",
       "4   0.042879  0.154414  0.881985  0.613308  0.094439  0.848292\n",
       "5   1.155759 -0.951931  1.115433  1.081285 -0.874828  0.564274\n",
       "6   0.505356  0.136741  1.036721  0.070874 -0.928161  0.500638\n",
       "7   1.213584  0.524270 -0.546613 -0.170270  0.021695  0.721247\n",
       "8  -1.173564  0.339249 -0.859683  1.063074 -1.042430  0.022618\n",
       "9  -0.150762 -0.047006  0.808330  0.573355  0.497316  0.215781\n",
       "10  0.167154  0.006036  0.766078 -0.774527  0.310309  0.717661\n",
       "11  0.180043 -0.559716  0.037415 -0.232565  0.746163 -0.870061\n",
       "12 -0.694271 -1.243527 -0.630003 -0.708625 -0.653024 -0.121635\n",
       "13  0.281620  1.233001 -0.594069 -0.821473  0.220906  0.777587\n",
       "14 -0.932143 -0.289536  0.450891  1.203512  0.765258 -0.606068\n",
       "15  0.235748 -0.415700 -0.997427 -0.277788 -0.040600 -0.751170\n",
       "16  0.655208  0.715417 -0.119034 -0.446623  1.272396  0.518244\n",
       "17 -1.019531 -2.378617  1.802843 -1.772659 -0.316467  0.985079\n",
       "18  0.749305  0.421106  0.535920  0.519077  0.987311 -0.042310\n",
       "19 -0.941119  0.254164  0.374065  0.227912  0.344252 -0.607863\n",
       "20  0.093666 -0.316261 -0.135934 -0.614475  0.835061  0.385332\n",
       "21 -0.479980 -0.620301 -0.935197  0.365424 -0.801617 -0.795194\n",
       "22 -0.152475 -0.015360 -0.282896  0.302966  0.961536 -0.360577\n",
       "23  0.842410  0.419068 -0.649329  0.291106  0.675934  0.333390\n",
       "24  0.839375 -0.826415  0.253794 -0.333662  0.600911  1.046372\n",
       "25  0.854434 -0.452231  0.233038  0.305643  0.111555  0.047781\n",
       "26  0.557987 -1.248075  1.369917 -0.348699 -1.192376  0.819762\n",
       "27  0.460134  0.597558  0.349213  0.329658  0.600580 -0.047959\n",
       "28  0.419364  0.439031 -0.176980 -0.324308 -0.087381  0.182621\n",
       "29  0.445397 -0.427448  0.516395 -0.742909 -0.161144 -0.442624\n",
       "..       ...       ...       ...       ...       ...       ...\n",
       "51 -0.095972 -0.032877 -0.423947  0.972194 -0.488823 -0.143386\n",
       "52  0.446892 -1.507813 -0.645050  1.873589 -2.073565  0.937493\n",
       "53 -1.294172 -1.205483  0.678547 -0.750763 -0.416279  0.997814\n",
       "54 -1.151571  0.137427  0.566030 -0.732969 -0.178841  0.746598\n",
       "55 -0.338517 -0.397203 -0.109926  0.316335 -0.958326 -0.578976\n",
       "56  0.754814  0.428090 -0.240482  0.250419 -0.162731  0.696982\n",
       "57 -1.079742 -0.963979 -0.062609  0.484420 -0.294141 -0.204385\n",
       "58  0.735044  0.062679  0.211430  0.886332 -0.871089 -0.012380\n",
       "59 -0.548946 -0.446550 -0.308555  0.128953 -0.459535 -0.076725\n",
       "60 -0.640384  0.401236  0.863009  0.381935 -0.569968  0.599959\n",
       "61  0.361599  0.946000  0.862303 -0.381916 -0.700446  0.694223\n",
       "62 -0.326819 -0.558281 -0.801178  0.815710  0.113441  0.073539\n",
       "63  0.332621  0.081485  0.158249 -0.020840  0.460466 -0.594142\n",
       "64  0.148344  0.333924  0.357134 -0.133556 -0.677442 -0.282914\n",
       "65  0.669950  1.192698  0.742461 -0.333723 -0.921895  0.766686\n",
       "66 -0.479485 -0.486320  0.079525 -0.117038  0.074168  0.173929\n",
       "67 -0.222652  0.799349  0.972326  0.382260 -0.063726  0.198309\n",
       "68 -0.755817 -0.162976 -0.220533 -0.269564  0.682552  0.414198\n",
       "69 -0.281772  0.715849  0.083670 -1.193940  0.196216  0.151308\n",
       "70 -0.132035  0.018131  0.176987 -0.418749  0.239076  0.604415\n",
       "71 -0.526967 -2.054435 -0.138487 -0.640927  0.379745 -0.542073\n",
       "72  0.583725 -0.753574 -1.182256  0.039511  0.360337  0.306739\n",
       "73  0.621524  0.223446  0.057783  0.350637  0.776465 -0.262974\n",
       "74  0.260777  0.689511 -0.858864 -0.105743  0.035360 -0.798797\n",
       "75  0.546500  0.699077  0.147395 -1.473438 -0.533398 -0.379967\n",
       "76 -0.568421  0.023650  0.119810 -0.726894 -0.501216 -0.115364\n",
       "77  1.179620  0.507862  0.160949 -0.840718 -0.673402  0.173014\n",
       "78 -0.024664 -0.032842 -0.873934 -0.118272  0.371869 -1.120477\n",
       "79  0.403405  0.725065 -0.820760  0.180681  1.077428  0.046858\n",
       "80  0.237640  0.456697 -0.651158 -1.286736 -0.163607  0.626159\n",
       "\n",
       "[81 rows x 6 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create DataFrames for the final weights going into and out of each hidden node\n",
    "\n",
    "HiddenToOutputFinalWeights = pd.DataFrame([vWeightArrayPost[0]\n",
    "                                          ,vWeightArrayPost[1]\n",
    "                                          ,vWeightArrayPost[2]\n",
    "                                          ,vWeightArrayPost[3]\n",
    "                                          ,vWeightArrayPost[4]\n",
    "                                          ,vWeightArrayPost[5]\n",
    "                                          ,vWeightArrayPost[6]\n",
    "                                          ,vWeightArrayPost[7]\n",
    "                                          ,vWeightArrayPost[8]\n",
    "                                          ], columns = [\"H0\", \"H1\", \"H2\", \"H3\"\n",
    "                                                       ,\"H4\",\"H5\"]\n",
    "                                         , index = [\"o0\", \"o1\", \"o2\", \"o3\", \"o4\"\n",
    "                                                   , \"o5\", \"o6\", \"o7\", \"o8\"])\n",
    "HiddenToOutputFinalWeights = HiddenToOutputFinalWeights.transpose()\n",
    "HiddenToOutputFinalWeights\n",
    "\n",
    "InputToHiddenFinalWeights = pd.DataFrame([wWeightArrayPost[0]\n",
    "                                          ,wWeightArrayPost[1]\n",
    "                                          ,wWeightArrayPost[2]\n",
    "                                          ,wWeightArrayPost[3]\n",
    "                                          ,wWeightArrayPost[4]\n",
    "                                          ,wWeightArrayPost[5]]\n",
    "                                          , index = [\"H0\", \"H1\", \"H2\", \"H3\"\n",
    "                                                       ,\"H4\",\"H5\"])\n",
    "InputToHiddenFinalWeights = InputToHiddenFinalWeights.transpose()\n",
    "\n",
    "InputToHiddenFinalWeights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R1BDnz6jFWfT"
   },
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rB9jEjxyFrBt"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 527
    },
    "colab_type": "code",
    "id": "qHFODzRjz6Qh",
    "outputId": "dc324485-3188-4fd9-e805-aea262fc861f"
   },
   "outputs": [],
   "source": [
    "HiddenToOutputWeightIndex = round(HiddenToOutputFinalWeights.div(HiddenToOutputFinalWeights.mean(axis=1), axis = 0)*-100,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "662vkGIJ2kZc"
   },
   "outputs": [],
   "source": [
    "HiddenToOutputWeightIndex.to_csv(\"HiddenToOutPutIndex.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>H0</th>\n",
       "      <th>H1</th>\n",
       "      <th>H2</th>\n",
       "      <th>H3</th>\n",
       "      <th>H4</th>\n",
       "      <th>H5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>A</th>\n",
       "      <td>0.007110</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.408216</td>\n",
       "      <td>0.974541</td>\n",
       "      <td>0.992894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B</th>\n",
       "      <td>0.998406</td>\n",
       "      <td>0.013639</td>\n",
       "      <td>0.630785</td>\n",
       "      <td>0.978649</td>\n",
       "      <td>0.075005</td>\n",
       "      <td>0.996177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C</th>\n",
       "      <td>0.999693</td>\n",
       "      <td>0.936441</td>\n",
       "      <td>0.880345</td>\n",
       "      <td>0.623670</td>\n",
       "      <td>0.613553</td>\n",
       "      <td>0.871032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D</th>\n",
       "      <td>0.997621</td>\n",
       "      <td>0.001144</td>\n",
       "      <td>0.997476</td>\n",
       "      <td>0.018711</td>\n",
       "      <td>0.959132</td>\n",
       "      <td>0.997297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E</th>\n",
       "      <td>0.997865</td>\n",
       "      <td>0.998076</td>\n",
       "      <td>0.002927</td>\n",
       "      <td>0.983801</td>\n",
       "      <td>0.584529</td>\n",
       "      <td>0.994715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F</th>\n",
       "      <td>0.970429</td>\n",
       "      <td>0.950696</td>\n",
       "      <td>0.042610</td>\n",
       "      <td>0.999705</td>\n",
       "      <td>0.487976</td>\n",
       "      <td>0.999150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>G</th>\n",
       "      <td>0.998149</td>\n",
       "      <td>0.064842</td>\n",
       "      <td>0.660514</td>\n",
       "      <td>0.941307</td>\n",
       "      <td>0.116788</td>\n",
       "      <td>0.962076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H</th>\n",
       "      <td>0.006801</td>\n",
       "      <td>0.003073</td>\n",
       "      <td>0.021630</td>\n",
       "      <td>0.412740</td>\n",
       "      <td>0.917719</td>\n",
       "      <td>0.992818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I</th>\n",
       "      <td>0.999384</td>\n",
       "      <td>0.976770</td>\n",
       "      <td>0.860180</td>\n",
       "      <td>0.037118</td>\n",
       "      <td>0.009933</td>\n",
       "      <td>0.989507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>J</th>\n",
       "      <td>0.987304</td>\n",
       "      <td>0.981389</td>\n",
       "      <td>0.830197</td>\n",
       "      <td>0.010164</td>\n",
       "      <td>0.011538</td>\n",
       "      <td>0.988122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>K</th>\n",
       "      <td>0.428659</td>\n",
       "      <td>0.099038</td>\n",
       "      <td>0.556683</td>\n",
       "      <td>0.985247</td>\n",
       "      <td>0.990624</td>\n",
       "      <td>0.006929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L</th>\n",
       "      <td>0.970113</td>\n",
       "      <td>0.932013</td>\n",
       "      <td>0.377531</td>\n",
       "      <td>0.024120</td>\n",
       "      <td>0.995049</td>\n",
       "      <td>0.038518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>M</th>\n",
       "      <td>0.003104</td>\n",
       "      <td>0.000099</td>\n",
       "      <td>0.997795</td>\n",
       "      <td>0.019286</td>\n",
       "      <td>0.990146</td>\n",
       "      <td>0.711042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N</th>\n",
       "      <td>0.004712</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.988784</td>\n",
       "      <td>0.001207</td>\n",
       "      <td>0.982770</td>\n",
       "      <td>0.995998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>O</th>\n",
       "      <td>0.995130</td>\n",
       "      <td>0.003905</td>\n",
       "      <td>0.999510</td>\n",
       "      <td>0.006589</td>\n",
       "      <td>0.928135</td>\n",
       "      <td>0.996000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P</th>\n",
       "      <td>0.937467</td>\n",
       "      <td>0.063712</td>\n",
       "      <td>0.612045</td>\n",
       "      <td>0.990552</td>\n",
       "      <td>0.214171</td>\n",
       "      <td>0.999861</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         H0        H1        H2        H3        H4        H5\n",
       "A  0.007110  0.037037  0.000961  0.408216  0.974541  0.992894\n",
       "B  0.998406  0.013639  0.630785  0.978649  0.075005  0.996177\n",
       "C  0.999693  0.936441  0.880345  0.623670  0.613553  0.871032\n",
       "D  0.997621  0.001144  0.997476  0.018711  0.959132  0.997297\n",
       "E  0.997865  0.998076  0.002927  0.983801  0.584529  0.994715\n",
       "F  0.970429  0.950696  0.042610  0.999705  0.487976  0.999150\n",
       "G  0.998149  0.064842  0.660514  0.941307  0.116788  0.962076\n",
       "H  0.006801  0.003073  0.021630  0.412740  0.917719  0.992818\n",
       "I  0.999384  0.976770  0.860180  0.037118  0.009933  0.989507\n",
       "J  0.987304  0.981389  0.830197  0.010164  0.011538  0.988122\n",
       "K  0.428659  0.099038  0.556683  0.985247  0.990624  0.006929\n",
       "L  0.970113  0.932013  0.377531  0.024120  0.995049  0.038518\n",
       "M  0.003104  0.000099  0.997795  0.019286  0.990146  0.711042\n",
       "N  0.004712  0.000090  0.988784  0.001207  0.982770  0.995998\n",
       "O  0.995130  0.003905  0.999510  0.006589  0.928135  0.996000\n",
       "P  0.937467  0.063712  0.612045  0.990552  0.214171  0.999861"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "postHidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Classifier NN Working Code",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
