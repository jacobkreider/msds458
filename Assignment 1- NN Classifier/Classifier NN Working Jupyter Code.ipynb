{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Classifier NN Working Code",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "VSAJFp6ItDAf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CacKZxD1tE61",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Starting with her code. Then going to split it apart below and reshape to give outputs I'm looking for\n"
      ]
    },
    {
      "metadata": {
        "id": "SVcXg4xhZEcH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from random import seed\n",
        "import random\n",
        "from random import randint\n",
        "import pandas as pd\n",
        "\n",
        "# We want to use the exp function (e to the x); \n",
        "# it's part of our transfer function definition\n",
        "from math import exp\n",
        "\n",
        "# Biting the bullet and starting to use NumPy for arrays\n",
        "import numpy as np\n",
        "\n",
        "# So we can make a separate list from an initial one\n",
        "import copy\n",
        "\n",
        "# For pretty-printing the arrays\n",
        "np.set_printoptions(precision=3)\n",
        "np.set_printoptions(suppress=True) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dpkUtOUp_Jbf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Some \"worker function\" that eist for specific tasks\n",
        "# Compute neuron activation using sigmoid transfer function\n",
        "def computeTransferFnctn(summedNeuronInput, alpha):\n",
        "    activation = 1.0 / (1.0 + exp(-alpha*summedNeuronInput)) \n",
        "    return activation\n",
        "    \n",
        "# Compute derivative of transfer function\n",
        "def computeTransferFnctnDeriv(NeuronOutput, alpha):\n",
        "    return alpha*NeuronOutput*(1.0 -NeuronOutput)     \n",
        "\n",
        "\n",
        "def matrixDotProduct (matrx1,matrx2):\n",
        "    dotProduct = np.dot(matrx1,matrx2)\n",
        "    \n",
        "    return(dotProduct) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-T2SLVrg_Jx9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Function to obtain the neural network size specifications\n",
        "\n",
        "def obtainNeuralNetworkSizeSpecs (): \n",
        "\n",
        "    numInputNodes = 81\n",
        "    numHiddenNodes = 6\n",
        "    numOutputNodes = 9   \n",
        "    print()\n",
        "    print(\"  The number of nodes at each level are:\")\n",
        "    print(\"    Input: 9x9 (square array)\")\n",
        "    print(\"    Hidden: \", numHiddenNodes)\n",
        "    print(\"    Output: \", numOutputNodes)\n",
        "            \n",
        "# We create a list containing the crucial SIZES for the connection weight arrays                \n",
        "    arraySizeList = (numInputNodes, numHiddenNodes, numOutputNodes)\n",
        "    \n",
        "# We return this list to the calling procedure, 'main'.       \n",
        "    return (arraySizeList)  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_3ylsOod_J9S",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Function to initialize a specific connection weight with a randomly-generated \n",
        "# number between 0 & 1\n",
        "\n",
        "def InitializeWeight ():\n",
        "\n",
        "    randomNum = random.random()\n",
        "    weight=1-2*randomNum\n",
        "#    print weight\n",
        "           \n",
        "    return (weight) \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kqfSikBc_KHI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Function to initialize the node-to-node connection weight arrays\n",
        "\n",
        "def initializeWeightArray (weightArraySizeList):\n",
        "    numLowerNodes = weightArraySizeList[0] \n",
        "    numUpperNodes = weightArraySizeList[1] \n",
        "   \n",
        "    weightArray = np.zeros((numUpperNodes,numLowerNodes))    # iniitalize the weight matrix with 0's\n",
        "    for row in range(numUpperNodes):  #  Number of rows in weightMatrix\n",
        "        # For an input-to-hidden weight matrix, the rows correspond to the number of hidden nodes\n",
        "        #    and the columns correspond to the number of input nodes.\n",
        "        #    This creates an HxI matrix, which can be multiplied by the input matrix (expressed as a column)\n",
        "        # Similarly, for a hidden-to-output matrix, the rows correspond to the number of output nodes.\n",
        "        for col in range(numLowerNodes):  # number of columns in matrix 2\n",
        "            weightArray[row,col] = InitializeWeight ()                 \n",
        "         \n",
        "    return (weightArray)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VKXQlvwt_KQ2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Function to initialize the bias weight arrays\n",
        "\n",
        "def initializeBiasWeightArray (numBiasNodes):\n",
        "    biasWeightArray = np.zeros(numBiasNodes)    # iniitalize the weight matrix with 0's\n",
        "    for node in range(numBiasNodes):  #  Number of nodes in bias weight set\n",
        "        biasWeightArray[node] = InitializeWeight ()\n",
        "      \n",
        "    return (biasWeightArray) \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "msjrTbl7AgbQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Function to return a trainingDataList\n",
        "\n",
        "def obtainSelectedAlphabetTrainingValues (dataSet):\n",
        "    \n",
        "# Note: Nine possible output classes: 0 .. 8 trainingDataListXX [4]    \n",
        "    trainingDataListA0 =  (1,[0,0,0,0,1,0,0,0,0, 0,0,0,1,0,1,0,0,0, 0,0,1,0,0,0,1,0,0\n",
        "                              , 0,1,0,0,0,0,0,1,0, 1,1,1,1,1,1,1,1,1, 1,0,0,0,0,0,0,0,1\n",
        "                              , 1,0,0,0,0,0,0,0,1, 1,0,0,0,0,0,0,0,1, 1,0,0,0,0,0,0,0,1]\n",
        "                           ,1,'A',0,'A') \n",
        "    trainingDataListB0 =  (2,[1,1,1,1,1,1,1,1,0, 1,0,0,0,0,0,0,0,1, 1,0,0,0,0,0,0,0,1\n",
        "                              , 1,0,0,0,0,0,0,1,0, 1,1,1,1,1,1,1,0,0, 1,0,0,0,0,0,0,1,0\n",
        "                              , 1,0,0,0,0,0,0,0,1, 1,0,0,0,0,0,0,0,1, 1,1,1,1,1,1,1,1,0]\n",
        "                           ,2,'B',1,'B') \n",
        "    trainingDataListC0 =  (3,[1,1,1,1,1,1,1,1,1, 1,0,0,0,0,0,0,0,0, 1,0,0,0,0,0,0,0,0\n",
        "                              , 1,0,0,0,0,0,0,0,0, 1,0,0,0,0,0,0,0,0, 1,0,0,0,0,0,0,0,0\n",
        "                              , 1,0,0,0,0,0,0,0,0, 1,0,0,0,0,0,0,0,0, 1,1,1,1,1,1,1,1,1]\n",
        "                           ,3,'C',2,'C') \n",
        "    trainingDataListD0 =  (4,[1,1,1,1,1,1,1,1,0, 1,0,0,0,0,0,0,1,1, 1,0,0,0,0,0,0,0,1\n",
        "                              , 1,0,0,0,0,0,0,0,1, 1,0,0,0,0,0,0,0,1, 1,0,0,0,0,0,0,0\n",
        "                              ,1, 1,0,0,0,0,0,0,0,1, 1,0,0,0,0,0,0,1,1, 1,1,1,1,1,1,1\n",
        "                              ,1,0],4,'D',3,'O') \n",
        "    trainingDataListE0 =  (5,[1,1,1,1,1,1,1,1,1, 1,0,0,0,0,0,0,0,0, 1,0,0,0,0,0,0,0,0\n",
        "                              , 1,0,0,0,0,0,0,0,0, 1,1,1,1,1,1,1,1,1, 1,0,0,0,0,0,0,0\n",
        "                              ,0, 1,0,0,0,0,0,0,0,0, 1,0,0,0,0,0,0,0,0, 1,1,1,1,1,1,1\n",
        "                              ,1,1],5,'E',4,'E') \n",
        "    trainingDataListF0 =  (6,[1,1,1,1,1,1,1,1,1, 1,0,0,0,0,0,0,0,0, 1,0,0,0,0,0,0,0,0\n",
        "                              , 1,0,0,0,0,0,0,0,0, 1,1,1,1,1,1,1,1,1, 1,0,0,0,0,0,0,0\n",
        "                              ,0, 1,0,0,0,0,0,0,0,0, 1,0,0,0,0,0,0,0,0, 1,0,0,0,0,0,0\n",
        "                              ,0,0],6,'F',4,'E') \n",
        "    trainingDataListG0 =  (7,[1,1,1,1,1,1,1,1,1, 1,0,0,0,0,0,0,0,0, 1,0,0,0,0,0,0,0,0\n",
        "                              , 1,0,0,0,0,0,0,0,0, 1,0,0,0,0,0,0,0,0, 1,0,0,0,0,0,1,1\n",
        "                              ,1, 1,0,0,0,0,0,0,0,1, 1,0,0,0,0,0,0,0,1, 1,1,1,1,1,1,1\n",
        "                              ,1,1],7,'G',1,'C')\n",
        "    trainingDataListH0 =  (8,[1,0,0,0,0,0,0,0,1, 1,0,0,0,0,0,0,0,1, 1,0,0,0,0,0,0,0,1\n",
        "                              , 1,0,0,0,0,0,0,0,1, 1,1,1,1,1,1,1,1,1, 1,0,0,0,0,0,0,0\n",
        "                              ,1, 1,0,0,0,0,0,0,0,1, 1,0,0,0,0,0,0,0,1, 1,0,0,0,0,0,0\n",
        "                              ,0,1],8,'H',0,'A') \n",
        "    trainingDataListI0 =  (9,[0,0,1,1,1,1,1,0,0, 0,0,0,0,1,0,0,0,0, 0,0,0,0,1,0,0,0,0\n",
        "                              , 0,0,0,0,1,0,0,0,0, 0,0,0,0,1,0,0,0,0, 0,0,0,0,1,0,0,0\n",
        "                              ,0, 0,0,0,0,1,0,0,0,0, 0,0,0,0,1,0,0,0,0, 0,0,1,1,1,1,1\n",
        "                              ,0,0],9,'I',5,'I') \n",
        "    trainingDataListJ0 = (10,[0,0,0,0,0,0,0,1,0, 0,0,0,0,0,0,0,1,0, 0,0,0,0,0,0,0,1,0\n",
        "                              , 0,0,0,0,0,0,0,1,0, 0,0,0,0,0,0,0,1,0, 0,1,0,0,0,0,0,1\n",
        "                              ,0, 0,1,0,0,0,0,0,1,0, 0,0,1,0,0,0,1,0,0, 0,0,0,1,1,1,0\n",
        "                              ,0,0],10,'J',5,'I') \n",
        "    trainingDataListK0 = (11,[1,0,0,0,0,0,1,0,0, 1,0,0,0,0,1,0,0,0, 1,0,0,0,1,0,0,0,0\n",
        "                              , 1,0,0,1,0,0,0,0,0, 1,1,1,0,0,0,0,0,0, 1,0,0,1,0,0,0,0\n",
        "                              ,0, 1,0,0,0,1,0,0,0,0, 1,0,0,0,0,1,0,0,0, 1,0,0,0,0,0,1\n",
        "                              ,0,0],11,'K',6,'K')    \n",
        "    trainingDataListL0 = (12,[1,0,0,0,0,0,0,0,0, 1,0,0,0,0,0,0,0,0, 1,0,0,0,0,0,0,0,0\n",
        "                              , 1,0,0,0,0,0,0,0,0, 1,0,0,0,0,0,0,0,0, 1,0,0,0,0,0,0,0\n",
        "                              ,0, 1,0,0,0,0,0,0,0,0, 1,0,0,0,0,0,0,0,0, 1,1,1,1,1,1,1\n",
        "                              ,1,1],12,'L',7,'L') \n",
        "    trainingDataListM0 = (13,[1,0,0,0,0,0,0,0,1, 1,1,0,0,0,0,0,1,1, 1,1,0,0,0,0,0,1,1\n",
        "                              , 1,0,1,0,0,0,1,0,1, 1,0,1,0,0,0,1,0,1, 1,0,0,1,0,1,0,0\n",
        "                              ,1, 1,0,0,1,0,1,0,0,1, 1,1,0,0,1,0,0,0,1, 1,0,0,0,1,0,0\n",
        "                              ,0,1],13,'M',8,'M')            \n",
        "    trainingDataListN0 = (14,[1,0,0,0,0,0,0,0,1, 1,1,0,0,0,0,0,0,1, 1,0,1,0,0,0,0,0,1\n",
        "                              , 1,0,0,1,0,0,0,0,1, 1,0,0,0,1,0,0,0,1, 1,0,0,0,0,1,0,0\n",
        "                              ,1, 1,0,0,0,0,0,1,0,1, 1,0,0,0,0,0,0,1,1, 1,0,0,0,0,0,0\n",
        "                              ,0,1],14,'N',8,'M') \n",
        "    trainingDataListO0 = (15,[0,1,1,1,1,1,1,1,0, 1,1,0,0,0,0,0,1,1, 1,0,0,0,0,0,0,0,1\n",
        "                              , 1,0,0,0,0,0,0,0,1, 1,0,0,0,0,0,0,0,1, 1,0,0,0,0,0,0,0\n",
        "                              ,1, 1,0,0,0,0,0,0,0,1, 1,0,0,0,0,0,0,0,1, 0,1,1,1,1,1,1\n",
        "                              ,1,0],15,'O',3,'O') \n",
        "    trainingDataListP0 = (16,[1,1,1,1,1,1,1,1,1, 1,0,0,0,0,0,0,0,1, 1,0,0,0,0,0,0,0,1\n",
        "                              , 1,0,0,0,0,0,0,0,1, 1,1,1,1,1,1,1,1,1, 1,0,0,0,0,0,0,0\n",
        "                              ,0, 1,0,0,0,0,0,0,0,0, 1,0,0,0,0,0,0,0,0, 1,0,0,0,0,0,0\n",
        "                              ,0,0],16,'P',1, 'B') \n",
        "\n",
        "\n",
        "    if dataSet == 1: trainingDataList = trainingDataListA0\n",
        "    if dataSet == 2: trainingDataList = trainingDataListB0 \n",
        "    if dataSet == 3: trainingDataList = trainingDataListC0\n",
        "    if dataSet == 4: trainingDataList = trainingDataListD0     \n",
        "    if dataSet == 5: trainingDataList = trainingDataListE0\n",
        "    if dataSet == 6: trainingDataList = trainingDataListF0 \n",
        "    if dataSet == 7: trainingDataList = trainingDataListG0 \n",
        "    if dataSet == 8: trainingDataList = trainingDataListH0\n",
        "    if dataSet == 9: trainingDataList = trainingDataListI0\n",
        "    if dataSet == 10: trainingDataList = trainingDataListJ0    \n",
        "\n",
        "    if dataSet == 11: trainingDataList = trainingDataListK0 \n",
        "    if dataSet == 12: trainingDataList = trainingDataListL0\n",
        "    if dataSet == 13: trainingDataList = trainingDataListM0\n",
        "    if dataSet == 14: trainingDataList = trainingDataListN0 \n",
        "    if dataSet == 15: trainingDataList = trainingDataListO0 \n",
        "    if dataSet == 16: trainingDataList = trainingDataListP0  \n",
        "\n",
        "                           \n",
        "    return (trainingDataList)  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pc08IcqjE-wf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Feedforward Pass"
      ]
    },
    {
      "metadata": {
        "id": "4kxVQ5pgeDyT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def ComputeSingleFeedforwardPassFirstStep (alpha, inputDataList, wWeightArray, biasHiddenWeightArray):\n",
        "    \n",
        "# iniitalize the sum of inputs into the hidden array with 0's  \n",
        "    sumIntoHiddenArray = np.zeros(hiddenArrayLength)    \n",
        "    hiddenArray = np.zeros(hiddenArrayLength)   \n",
        "\n",
        "    sumIntoHiddenArray = matrixDotProduct (wWeightArray,inputDataList)\n",
        "    \n",
        "    for node in range(hiddenArrayLength):  #  Number of hidden nodes\n",
        "        hiddenNodeSumInput=sumIntoHiddenArray[node]+biasHiddenWeightArray[node]\n",
        "        hiddenArray[node] = computeTransferFnctn(hiddenNodeSumInput, alpha)\n",
        "\n",
        "#    print ' '\n",
        "#    print 'Back in ComputeSingleFeedforwardPass'\n",
        "#    print 'The activations for the hidden nodes are:'\n",
        "#    print '  Hidden0 = %.4f' % hiddenActivation0, 'Hidden1 = %.4f' % hiddenActivation1\n",
        "\n",
        "                                                                                                    \n",
        "    return (hiddenArray);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Xe1MRrWed5V5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def ComputeSingleFeedforwardPassSecondStep (alpha, hiddenArray, vWeightArray, biasOutputWeightArray):\n",
        "    \n",
        "# initialize the sum of inputs into the hidden array with 0's  \n",
        "    sumIntoOutputArray = np.zeros(hiddenArrayLength)    \n",
        "    outputArray = np.zeros(outputArrayLength)   \n",
        "\n",
        "    sumIntoOutputArray = matrixDotProduct (vWeightArray,hiddenArray)\n",
        "    \n",
        "    for node in range(outputArrayLength):  #  Number of hidden nodes\n",
        "        outputNodeSumInput=sumIntoOutputArray[node]+biasOutputWeightArray[node]\n",
        "        outputArray[node] = computeTransferFnctn(outputNodeSumInput, alpha)\n",
        "                                                                                                   \n",
        "    return (outputArray)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "e_U-w2jCB7qT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "For the ComputeOutputsAcrossAllTrainingData in the next section, I actually call this outside of\n",
        "a function in a later section-- I never actually use the function. \n",
        "\n",
        "I found it easier to get the returns I wanted outside of a function call-- the same reason I did away with\n",
        "the 'Main' procedure. Obviously not what you want to do for production work, but for this it made my life a bit easier."
      ]
    },
    {
      "metadata": {
        "id": "xyXgCJyzAgq2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Procedure to compute the output node activations and determine errors across the entire training\n",
        "#  data set, and print results.\n",
        "\n",
        "def ComputeOutputsAcrossAllTrainingData (alpha, numTrainingDataSets, wWeightArray, \n",
        "biasHiddenWeightArray, vWeightArray, biasOutputWeightArray):\n",
        "\n",
        "    selectedTrainingDataSet = 1 \n",
        "                                \n",
        "                              \n",
        "    \n",
        "    allHiddenActivations = [] \n",
        "    while selectedTrainingDataSet < numTrainingDataSets + 1: \n",
        "        print()\n",
        "        print(\" the selected Training Data Set is \", selectedTrainingDataSet)\n",
        "        trainingDataList = obtainSelectedAlphabetTrainingValues (selectedTrainingDataSet)\n",
        " \n",
        "\n",
        "        trainingDataInputList = trainingDataList[1]      \n",
        "        \n",
        "        inputDataList = [] \n",
        "        for node in range(inputArrayLength): \n",
        "            trainingData = trainingDataInputList[node]  \n",
        "            inputDataList.append(trainingData)\n",
        "\n",
        "        letterNum = trainingDataList[2]\n",
        "        letterChar = trainingDataList[3]  \n",
        "        print()\n",
        "        print(\"  Data Set Number\", selectedTrainingDataSet, \" for letter \", letterChar, \" with letter number \", letterNum) \n",
        "\n",
        "        hiddenArray = ComputeSingleFeedforwardPassFirstStep (alpha, inputDataList, wWeightArray, biasHiddenWeightArray)\n",
        "\n",
        "        print()\n",
        "        print(\" The hidden node activations are: \")\n",
        "        print(hiddenArray)\n",
        "        \n",
        "\n",
        "        outputArray = ComputeSingleFeedforwardPassSecondStep (alpha, hiddenArray, vWeightArray, biasOutputWeightArray)\n",
        "    \n",
        "        print()\n",
        "        print(\" The output node activations are: \")\n",
        "        print(outputArray)   \n",
        "\n",
        "        desiredOutputArray = np.zeros(outputArrayLength)    # iniitalize the output array with 0's\n",
        "        desiredClass = trainingDataList[4]                 # identify the desired class\n",
        "        desiredOutputArray[desiredClass] = 1                # set the desired output for that class to 1\n",
        "     \n",
        "        print()\n",
        "        print(\" The desired output array values are: \")\n",
        "        print(desiredOutputArray)  \n",
        "     \n",
        "                        \n",
        "# Determine the error between actual and desired outputs\n",
        "\n",
        "# Initialize the error array\n",
        "        errorArray = np.zeros(outputArrayLength) \n",
        "    \n",
        "        newSSE = 0.0\n",
        "        for node in range(outputArrayLength):  #  Number of nodes in output set (classes)\n",
        "            errorArray[node] = desiredOutputArray[node] - outputArray[node]\n",
        "            newSSE = newSSE + errorArray[node]*errorArray[node]        \n",
        "\n",
        "        print()\n",
        "        print(\" ' The error values are:\")\n",
        "        print(errorArray)   \n",
        "        \n",
        "# Print the Summed Squared Error  \n",
        "        print(\"New SSE = %.6f\" % newSSE) \n",
        "         \n",
        "        selectedTrainingDataSet = selectedTrainingDataSet +1 "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pcwU9hylCfOo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Backpropagation"
      ]
    },
    {
      "metadata": {
        "id": "9j7OXyOwAg16",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Backpropagate weight changes onto the hidden-to-output connection weights\n",
        "\n",
        "def backpropagateOutputToHidden (alpha, eta, arraySizeList, errorArray, outputArray, hiddenArray, vWeightArray):\n",
        "\n",
        "# Unpack array lengths\n",
        "    hiddenArrayLength = arraySizeList [1]\n",
        "    outputArrayLength = arraySizeList [2]\n",
        "\n",
        "    transferFuncDerivArray = np.zeros(outputArrayLength)    \n",
        "      \n",
        "    for node in range(outputArrayLength):  #  Number of hidden nodes\n",
        "        transferFuncDerivArray[node]=computeTransferFnctnDeriv(outputArray[node], alpha)\n",
        "                        \n",
        "    deltaVWtArray = np.zeros((outputArrayLength, hiddenArrayLength))  # initialize an array for the deltas\n",
        "    newVWeightArray = np.zeros((outputArrayLength, hiddenArrayLength)) # initialize an array for the new hidden weights\n",
        "        \n",
        "    for row in range(outputArrayLength):  #  Number of rows in weightMatrix\n",
        "\n",
        "        for col in range(hiddenArrayLength):  # number of columns in weightMatrix\n",
        "            partialSSE_w_V_Wt = -errorArray[row]*transferFuncDerivArray[row]*hiddenArray[col]\n",
        "            deltaVWtArray[row,col] = -eta*partialSSE_w_V_Wt\n",
        "            newVWeightArray[row,col] = vWeightArray[row,col] + deltaVWtArray[row,col]                                                                                        \n",
        "                                                                  \n",
        "                                                                                                                                                                                                            \n",
        "    return (newVWeightArray)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3UxImgqeAhAX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Backpropagate weight changes onto the bias-to-output connection weights\n",
        "\n",
        "def backpropagateBiasOutputWeights (alpha, eta, arraySizeList, errorArray, outputArray, biasOutputWeightArray):\n",
        "\n",
        "#  Unpack the output array length\n",
        "    outputArrayLength = arraySizeList [2]\n",
        "\n",
        "    deltaBiasOutputArray = np.zeros(outputArrayLength)  # initialize an array for the deltas\n",
        "    newBiasOutputWeightArray = np.zeros(outputArrayLength) # initialize an array for the new output bias weights\n",
        "    transferFuncDerivArray = np.zeros(outputArrayLength)    # iniitalize an array for the transfer function\n",
        "      \n",
        "    for node in range(outputArrayLength):  #  Number of hidden nodes\n",
        "        transferFuncDerivArray[node]=computeTransferFnctnDeriv(outputArray[node], alpha)\n",
        " \n",
        "\n",
        "    for node in range(outputArrayLength):  #  Number of nodes in output array (same as number of output bias nodes)    \n",
        "        partialSSE_w_BiasOutput = -errorArray[node]*transferFuncDerivArray[node]\n",
        "        deltaBiasOutputArray[node] = -eta*partialSSE_w_BiasOutput  \n",
        "        newBiasOutputWeightArray[node] =  biasOutputWeightArray[node] + deltaBiasOutputArray[node]           \n",
        "                                                                                                          \n",
        "    return (newBiasOutputWeightArray)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xV2U-CR8C8ST",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Backpropagate weight changes onto the input-to-hidden connection weights\n",
        "\n",
        "def backpropagateHiddenToInput (alpha, eta, arraySizeList, errorArray, outputArray, hiddenArray,\n",
        "    inputArray, vWeightArray, wWeightArray, biasHiddenWeightArray, biasOutputWeightArray):\n",
        "\n",
        "# Unpack array lengths\n",
        "    inputArrayLength = arraySizeList [0]\n",
        "    hiddenArrayLength = arraySizeList [1]\n",
        "    outputArrayLength = arraySizeList [2]              \n",
        "                                          \n",
        "\n",
        "    transferFuncDerivHiddenArray = np.zeros(hiddenArrayLength)    \n",
        "    for node in range(hiddenArrayLength):  #  Number of hidden nodes\n",
        "        transferFuncDerivHiddenArray[node]=computeTransferFnctnDeriv(hiddenArray[node], alpha)\n",
        "        \n",
        "    errorTimesTFuncDerivOutputArray = np.zeros(outputArrayLength) # initialize array\n",
        "    transferFuncDerivOutputArray    = np.zeros(outputArrayLength) # initialize array\n",
        "    weightedErrorArray              = np.zeros(hiddenArrayLength) # initialize array\n",
        "      \n",
        "    for outputNode in range(outputArrayLength):  #  Number of output nodes\n",
        "        transferFuncDerivOutputArray[outputNode]=computeTransferFnctnDeriv(outputArray[outputNode], alpha)\n",
        "        errorTimesTFuncDerivOutputArray[outputNode] = errorArray[outputNode]*transferFuncDerivOutputArray[outputNode]\n",
        "        \n",
        "    for hiddenNode in range(hiddenArrayLength):\n",
        "        weightedErrorArray[hiddenNode] = 0\n",
        "        for outputNode in range(outputArrayLength):  #  Number of output nodes    \n",
        "            weightedErrorArray[hiddenNode] = weightedErrorArray[hiddenNode] \\\n",
        "            + vWeightArray[outputNode, hiddenNode]*errorTimesTFuncDerivOutputArray[outputNode]\n",
        "             \n",
        "    deltaWWtArray = np.zeros((hiddenArrayLength, inputArrayLength))  # initialize an array for the deltas\n",
        "    newWWeightArray = np.zeros((hiddenArrayLength, inputArrayLength)) # initialize an array for the new input-to-hidden weights\n",
        "        \n",
        "    for row in range(hiddenArrayLength):  \n",
        "\n",
        "        for col in range(inputArrayLength):  # number of columns in weightMatrix\n",
        "            partialSSE_w_W_Wts = -transferFuncDerivHiddenArray[row]*inputArray[col]*weightedErrorArray[row]\n",
        "            deltaWWtArray[row,col] = -eta*partialSSE_w_W_Wts\n",
        "            newWWeightArray[row,col] = wWeightArray[row,col] + deltaWWtArray[row,col]                                                                                     \n",
        "                                        \n",
        "    return (newWWeightArray)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RFpqA5ptC8YV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Backpropagate weight changes onto the bias-to-hidden connection weights\n",
        "\n",
        "def backpropagateBiasHiddenWeights (alpha, eta, arraySizeList, errorArray, outputArray, hiddenArray,\n",
        "    inputArray, vWeightArray, wWeightArray, biasHiddenWeightArray, biasOutputWeightArray):\n",
        "# Unpack array lengths\n",
        "    inputArrayLength = arraySizeList [0]\n",
        "    hiddenArrayLength = arraySizeList [1]\n",
        "    outputArrayLength = arraySizeList [2]  \n",
        "               \n",
        "\n",
        "    errorTimesTFuncDerivOutputArray = np.zeros(outputArrayLength)    \n",
        "    transferFuncDerivOutputArray    = np.zeros(outputArrayLength) \n",
        "    weightedErrorArray              = np.zeros(hiddenArrayLength)    \n",
        "\n",
        "    transferFuncDerivHiddenArray = np.zeros(hiddenArrayLength)  \n",
        "    partialSSE_w_BiasHidden      = np.zeros(hiddenArrayLength)  \n",
        "    deltaBiasHiddenArray         = np.zeros(hiddenArrayLength)  \n",
        "    newBiasHiddenWeightArray     = np.zeros(hiddenArrayLength)  \n",
        "          \n",
        "    for node in range(hiddenArrayLength):  #  Number of hidden nodes\n",
        "        transferFuncDerivHiddenArray[node]=computeTransferFnctnDeriv(hiddenArray[node], alpha)      \n",
        "                  \n",
        "    for outputNode in range(outputArrayLength):  #  Number of output nodes\n",
        "        transferFuncDerivOutputArray[outputNode]=computeTransferFnctnDeriv(outputArray[outputNode], alpha) \n",
        "        errorTimesTFuncDerivOutputArray[outputNode] = errorArray[outputNode]*transferFuncDerivOutputArray[outputNode]\n",
        "\n",
        "    for hiddenNode in range(hiddenArrayLength):\n",
        "        weightedErrorArray[hiddenNode] = 0\n",
        "        for outputNode in range(outputArrayLength):  #  Number of output nodes    \n",
        "            weightedErrorArray[hiddenNode] = (weightedErrorArray[hiddenNode]\n",
        "            + vWeightArray[outputNode, hiddenNode]*errorTimesTFuncDerivOutputArray[outputNode])\n",
        "\n",
        "    for hiddenNode in range(hiddenArrayLength):  #  Number of rows in input-to-hidden weightMatrix           \n",
        "        partialSSE_w_BiasHidden[hiddenNode] = -transferFuncDerivHiddenArray[hiddenNode]*weightedErrorArray[hiddenNode]\n",
        "        deltaBiasHiddenArray[hiddenNode] = -eta*partialSSE_w_BiasHidden[hiddenNode]\n",
        "        newBiasHiddenWeightArray[hiddenNode] = biasHiddenWeightArray[hiddenNode] + deltaBiasHiddenArray[hiddenNode]                                                                                                                                                                                                                                                         \n",
        "  \n",
        "                                                                                                                                            \n",
        "    return (newBiasHiddenWeightArray)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4P9IgcNAEjzP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### The 'Main' Procedure-- split out into separate code snippets"
      ]
    },
    {
      "metadata": {
        "id": "TcxHx5HZd4Mt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Define the global variables        \n",
        "    global inputArrayLength\n",
        "    global hiddenArrayLength\n",
        "    global outputArrayLength\n",
        "    global gridWidth\n",
        "    global gridHeight\n",
        "    global eGH # expandedGridHeight, defined in function expandLetterBoundaries \n",
        "    global eGW # expandedGridWidth defined in function expandLetterBoundaries \n",
        "    global mask1    \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fdqdOIvLiLlz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "outputId": "9c6769c0-7858-423a-945f-3fd225920929"
      },
      "cell_type": "code",
      "source": [
        "arraySizeList = list() # empty list\n",
        "\n",
        "# Obtain the actual sizes for each layer of the network       \n",
        "arraySizeList = obtainNeuralNetworkSizeSpecs ()\n",
        "    \n",
        "# Unpack the list; ascribe the various elements of the list to the sizes of different network layers\n",
        "# Note: A word on Python encoding ... the actually length of the array, in each of these three cases, \n",
        "#       will be xArrayLength. For example, the inputArrayLength for the 9x9 pixel array is 81. \n",
        "#       These values are passed to various procedures. They start filling in actual array values,\n",
        "#       where the array values start their count at element 0. However, when filling them in using a\n",
        "#       \"for node in range[limit]\" statement, the \"for\" loop fills from 0 up to limit-1. Thus, the\n",
        "#       original xArrayLength size is preserved.   \n",
        "inputArrayLength = arraySizeList [0] \n",
        "hiddenArrayLength = arraySizeList [1] \n",
        "outputArrayLength = arraySizeList [2] \n",
        "    \n",
        "print()\n",
        "print(\" inputArrayLength = \", inputArrayLength)\n",
        "print(\" hiddenArrayLength = \", hiddenArrayLength)\n",
        "print(\" outputArrayLength = \", outputArrayLength)        \n",
        "\n",
        "\n",
        "# Parameter definitions for backpropagation, to be replaced with user inputs\n",
        "alpha = 1.0\n",
        "eta = 0.5    \n",
        "maxNumIterations = 5000    # temporarily set to 10 for testing\n",
        "epsilon = 0.01\n",
        "iteration = 0\n",
        "SSE = 0.0\n",
        "numTrainingDataSets = 16\n",
        "allHiddenActivations = []"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  The number of nodes at each level are:\n",
            "    Input: 9x9 (square array)\n",
            "    Hidden:  6\n",
            "    Output:  9\n",
            "\n",
            " inputArrayLength =  81\n",
            " hiddenArrayLength =  6\n",
            " outputArrayLength =  9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "thk64IqjlMXV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Initialize the weight arrays for two sets of weights; w: input-to-hidden, and v: hidden-to-output\n",
        "####################################################################################################                \n",
        "seed(79)\n",
        "\n",
        "#\n",
        "# The wWeightArray is for Input-to-Hidden\n",
        "# The vWeightArray is for Hidden-to-Output\n",
        "\n",
        "wWeightArraySizeList = (inputArrayLength, hiddenArrayLength)\n",
        "vWeightArraySizeList = (hiddenArrayLength, outputArrayLength)\n",
        "biasHiddenWeightArraySize = hiddenArrayLength\n",
        "biasOutputWeightArraySize = outputArrayLength        \n",
        "\n",
        "# The node-to-node connection weights are stored in a 2-D array\n",
        "\n",
        "wWeightArray = initializeWeightArray (wWeightArraySizeList)\n",
        "  \n",
        "vWeightArray = initializeWeightArray (vWeightArraySizeList)\n",
        "\n",
        "# The bias weights are stored in a 1-D array         \n",
        "biasHiddenWeightArray = initializeBiasWeightArray (biasHiddenWeightArraySize)\n",
        "biasOutputWeightArray = initializeBiasWeightArray (biasOutputWeightArraySize) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4-Jv12Aalrfa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# This is the code from ComputeOutputsAcrossAllTrainingData, modified for\n",
        "# the outputs I am looking for\n",
        "\n",
        "selectedTrainingDataSet = 1 \n",
        "seed(79)\n",
        "                                \n",
        "                              \n",
        "    \n",
        "allHiddenActivationsPreLearn = np.array([])\n",
        "allOutputActivationsPreLearn = np.array([])\n",
        "desiredOutput = np.array([])\n",
        "\n",
        "while selectedTrainingDataSet < numTrainingDataSets + 1: \n",
        "    #print()\n",
        "    #print(\" the selected Training Data Set is \", selectedTrainingDataSet)\n",
        "    trainingDataList = obtainSelectedAlphabetTrainingValues (selectedTrainingDataSet)\n",
        "# Note: the trainingDataList is a list comprising several values:\n",
        "#    - the 0th is the list number \n",
        "#    - the 1st is the actual list of the input training values\n",
        "#    - etc. \n",
        "\n",
        "    trainingDataInputList = trainingDataList[1]      \n",
        "        \n",
        "    inputDataList = [] \n",
        "    for node in range(inputArrayLength): \n",
        "        trainingData = trainingDataInputList[node]  \n",
        "        inputDataList.append(trainingData)\n",
        "\n",
        "    letterNum = trainingDataList[2]\n",
        "    letterChar = trainingDataList[3]  \n",
        "    #print()\n",
        "    #print(\"  Data Set Number\", selectedTrainingDataSet, \" for letter \", letterChar, \" with letter number \", letterNum) \n",
        "\n",
        "    hiddenArray = np.array(ComputeSingleFeedforwardPassFirstStep (alpha, inputDataList, wWeightArray, biasHiddenWeightArray))\n",
        "    allHiddenActivationsPreLearn = np.append(allHiddenActivationsPreLearn, hiddenArray)\n",
        "    \n",
        "    outputArray = ComputeSingleFeedforwardPassSecondStep (alpha, hiddenArray, vWeightArray, biasOutputWeightArray)\n",
        "    allOutputActivationsPreLearn = np.append(allOutputActivationsPreLearn, outputArray)\n",
        "    \n",
        "    desiredOutputArray = np.zeros(outputArrayLength)    # iniitalize the output array with 0's\n",
        "    desiredClass = trainingDataList[4]                 # identify the desired class\n",
        "    desiredOutputArray[desiredClass] = 1\n",
        "    desiredOutput = np.append(desiredOutput, desiredOutputArray)\n",
        "    \n",
        "    \n",
        "    \n",
        "    selectedTrainingDataSet = selectedTrainingDataSet +1\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_gsGSPyXpafY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "3645bf90-272f-47fb-c46c-915ba95adc97"
      },
      "cell_type": "code",
      "source": [
        "# Perform backpropagation during each iteration\n",
        "# This code pulled from 'Main' and modified to return the new weights for use\n",
        "# outside of the loop\n",
        "\n",
        "vWeightArrayPost = np.array([])\n",
        "wWeightArrayPost = np.array([])\n",
        "biasHiddenWeightsPost = np.array([])\n",
        "biasOutputWeightsPost = np.array([])\n",
        "\n",
        "while iteration < maxNumIterations: \n",
        "    \n",
        "    # Increment the iteration count\n",
        "    iteration = iteration +1\n",
        "    \n",
        "    vWeightArrayPost = np.array([]) # Re-initializing the new weight arrays \n",
        "    wWeightArrayPost = np.array([]) # so they only contain the final weights\n",
        "    biasHiddenWeightsPost = np.array([])\n",
        "    biasOutputWeightsPost = np.array([])\n",
        "                          \n",
        "    # Re-initialize the training list at the start of each iteration\n",
        "    trainingDataList = (0,0,0,0,0,0,0,0,0\n",
        "                        , 0,0,0,0,0,0,0,0,0, 0,0,0,0,0,0,0,0,0, 0,0,0,0,0,0,0,0,0\n",
        "                        , 0,0,0,0,0,0,0,0,0, 0,0,0,0,0,0,0,0,0, 0,0,0,0,0,0,0,0,0\n",
        "                        , 0,0,0,0,0,0,0,0,0, 0,0,0,0,0,0,0,0,0, 0, ' ')\n",
        "    # Populate the training list with a random data set\n",
        "    dataSet = random.randint(1, numTrainingDataSets)\n",
        "    trainingDataList = obtainSelectedAlphabetTrainingValues(dataSet)\n",
        "    \n",
        "    # Create an input array based on the input training data list\n",
        "    inputDataList = []\n",
        "    inputDataArray = np.zeros(inputArrayLength)\n",
        "    \n",
        "    # Use the items in index 1 as the training inputs\n",
        "    thisTrainingDataList = list()\n",
        "    thisTrainingDataList = trainingDataList[1]\n",
        "    \n",
        "    for node in range(inputArrayLength):\n",
        "      trainingData = thisTrainingDataList[node]\n",
        "      inputDataList.append(trainingData)\n",
        "      inputDataArray[node] = trainingData\n",
        "      \n",
        "    # Create desired output array, from 4th element\n",
        "    desiredOutputArray = np.zeros(outputArrayLength)\n",
        "    desiredClass = trainingDataList[4]\n",
        "    desiredOutputArray[desiredClass] = 1\n",
        "    \n",
        "    # Compute feedforward pass\n",
        "    hiddenArray = ComputeSingleFeedforwardPassFirstStep (alpha, inputDataArray\n",
        "                                                         , wWeightArray\n",
        "                                                         , biasHiddenWeightArray)\n",
        "    outputArray = ComputeSingleFeedforwardPassSecondStep (alpha, hiddenArray\n",
        "                                                          ,vWeightArray\n",
        "                                                          , biasOutputWeightArray)\n",
        "    # Initialize the rror array\n",
        "    errorArray = np.zeros(outputArrayLength)\n",
        "    \n",
        "    # Determine the error and fill the array plus calculate new SSE\n",
        "    newSSE = 0.0\n",
        "    for node in range(outputArrayLength):\n",
        "      errorArray[node] = desiredOutputArray[node] - outputArray[node]\n",
        "      newSSE += errorArray[node]*errorArray[node]\n",
        "      \n",
        "    # Backpropagation\n",
        "    \n",
        "    # Ouput to Hidden backprop\n",
        "    newVWeightArray = backpropagateOutputToHidden (alpha, eta, arraySizeList\n",
        "                                                   , errorArray, outputArray\n",
        "                                                   , hiddenArray, vWeightArray)\n",
        "    \n",
        "    \n",
        "    newBiasOutputWeightArray = backpropagateBiasOutputWeights (alpha, eta\n",
        "                                                               , arraySizeList\n",
        "                                                               , errorArray\n",
        "                                                               , outputArray\n",
        "                                                               , biasOutputWeightArray) \n",
        "    # Hidden to Input backprop\n",
        "    newWWeightArray = backpropagateHiddenToInput (alpha, eta, arraySizeList\n",
        "                                                  , errorArray, outputArray\n",
        "                                                  , hiddenArray, inputDataList\n",
        "                                                  , vWeightArray, wWeightArray\n",
        "                                                  , biasHiddenWeightArray\n",
        "                                                  , biasOutputWeightArray)\n",
        "    newBiasHiddenWeightArray = backpropagateBiasHiddenWeights (alpha, eta\n",
        "                                                               , arraySizeList\n",
        "                                                               , errorArray\n",
        "                                                               , outputArray\n",
        "                                                               , hiddenArray\n",
        "                                                               , inputDataList\n",
        "                                                               , vWeightArray\n",
        "                                                               , wWeightArray\n",
        "                                                               , biasHiddenWeightArray\n",
        "                                                               , biasOutputWeightArray)\n",
        "    \n",
        "    # Update the weight and bias matrices\n",
        "    # Hidden-to-output update\n",
        "    vWeightArray = newVWeightArray[:]\n",
        "    \n",
        "    \n",
        "    biasOutputWeightArray = newBiasOutputWeightArray[:]\n",
        "    \n",
        "    # Input-to-hidden update\n",
        "    wWeightArray = newWWeightArray[:]  \n",
        "    \n",
        "    biasHiddenWeightArray = newBiasHiddenWeightArray[:] \n",
        "    \n",
        "    # Perform a forward pass with the new weights\n",
        "    hiddenArray = ComputeSingleFeedforwardPassFirstStep (alpha, inputDataList\n",
        "                                                         , wWeightArray\n",
        "                                                         , biasHiddenWeightArray)\n",
        "    outputArray = ComputeSingleFeedforwardPassSecondStep (alpha, hiddenArray\n",
        "                                                          , vWeightArray\n",
        "                                                          , biasOutputWeightArray)\n",
        "    \n",
        "    \n",
        "    # Check the new SSE\n",
        "    newSSE = 0.0\n",
        "    for node in range(outputArrayLength):\n",
        "      errorArray[node] - desiredOutputArray[node] - outputArray[node]\n",
        "      newSSE += errorArray[node]*errorArray[node]\n",
        "      \n",
        "    if newSSE < epsilon:\n",
        "      break\n",
        "# Append to our w weight array\n",
        "vWeightArrayPost = vWeightArray\n",
        "wWeightArrayPost = wWeightArray\n",
        "biasOutputWeightsPost = biasOutputWeightArray\n",
        "biasHiddenWeightsPost = biasHiddenWeightArray\n",
        "print(\"Out of while loop at iteration \", iteration)\n",
        "    \n",
        "\n",
        "    \n",
        "    \n",
        "  \n",
        "  \n",
        " "
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Out of while loop at iteration  4067\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TAcz4fLNnCuz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "62e9d5c4-cef0-4293-e08f-2266b6a54b61"
      },
      "cell_type": "code",
      "source": [
        "print()\n",
        "print(\"  After training:\")  \n",
        "\n",
        "# This is the code from ComputeOutputsAcrossAllTrainingData, modified for\n",
        "# the outputs I am looking for\n",
        "\n",
        "selectedTrainingDataSet = 1 \n",
        "seed(79)\n",
        "                                \n",
        "                              \n",
        "    \n",
        "allHiddenActivationsPostLearn = np.array([])\n",
        "allOutputActivationsPostLearn = np.array([])\n",
        "desiredOutputPost = np.array([])\n",
        "\n",
        "while selectedTrainingDataSet < numTrainingDataSets + 1: \n",
        "    #print()\n",
        "    #print(\" the selected Training Data Set is \", selectedTrainingDataSet)\n",
        "    trainingDataList = obtainSelectedAlphabetTrainingValues (selectedTrainingDataSet)\n",
        "# Note: the trainingDataList is a list comprising several values:\n",
        "#    - the 0th is the list number \n",
        "#    - the 1st is the actual list of the input training values\n",
        "#    - etc. \n",
        "\n",
        "    trainingDataInputList = trainingDataList[1]      \n",
        "        \n",
        "    inputDataList = [] \n",
        "    for node in range(inputArrayLength): \n",
        "        trainingData = trainingDataInputList[node]  \n",
        "        inputDataList.append(trainingData)\n",
        "\n",
        "    letterNum = trainingDataList[2]\n",
        "    letterChar = trainingDataList[3]  \n",
        "    #print()\n",
        "    #print(\"  Data Set Number\", selectedTrainingDataSet, \" for letter \", letterChar, \" with letter number \", letterNum) \n",
        "\n",
        "    hiddenArray = np.array(ComputeSingleFeedforwardPassFirstStep (alpha, inputDataList, wWeightArrayPost, biasHiddenWeightsPost))\n",
        "    allHiddenActivationsPostLearn = np.append(allHiddenActivationsPostLearn, hiddenArray)\n",
        "    \n",
        "    outputArray = ComputeSingleFeedforwardPassSecondStep (alpha, hiddenArray, vWeightArrayPost, biasOutputWeightsPost)\n",
        "    allOutputActivationsPostLearn = np.append(allOutputActivationsPostLearn, outputArray)\n",
        "    \n",
        "    desiredOutputArray = np.zeros(outputArrayLength)    # iniitalize the output array with 0's\n",
        "    desiredClass = trainingDataList[4]                 # identify the desired class\n",
        "    desiredOutputArray[desiredClass] = 1\n",
        "    desiredOutputPost = np.append(desiredOutputPost, desiredOutputArray)\n",
        "    \n",
        "    \n",
        "    \n",
        "    selectedTrainingDataSet = selectedTrainingDataSet +1\n",
        "    \n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  After training:\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "YlCZZjPxF_Zw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Output"
      ]
    },
    {
      "metadata": {
        "id": "whjW1r8em_Ft",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create dataframes for pre-training values of all activations\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "preHidden = pd.DataFrame([allHiddenActivationsPreLearn[0:6]\n",
        "                         ,allHiddenActivationsPreLearn[6:12]\n",
        "                         ,allHiddenActivationsPreLearn[12:18]\n",
        "                         ,allHiddenActivationsPreLearn[18:24]\n",
        "                         ,allHiddenActivationsPreLearn[24:30]\n",
        "                         ,allHiddenActivationsPreLearn[30:36]\n",
        "                         ,allHiddenActivationsPreLearn[36:42]\n",
        "                         ,allHiddenActivationsPreLearn[42:48]\n",
        "                         ,allHiddenActivationsPreLearn[48:54]\n",
        "                         ,allHiddenActivationsPreLearn[54:60]\n",
        "                         ,allHiddenActivationsPreLearn[60:66]\n",
        "                         ,allHiddenActivationsPreLearn[66:72]\n",
        "                         ,allHiddenActivationsPreLearn[72:78]\n",
        "                         ,allHiddenActivationsPreLearn[78:84]\n",
        "                         ,allHiddenActivationsPreLearn[84:90]\n",
        "                         ,allHiddenActivationsPreLearn[90:96]]\n",
        "                         , index = [\"A\", \"B\", \"C\", \"D\"\n",
        "                                   ,\"E\", \"F\", \"G\", \"H\"\n",
        "                                   ,\"I\", \"J\", \"K\", \"L\"\n",
        "                                   ,\"M\", \"N\", \"O\", \"P\"]\n",
        "                        , columns = [\"H0\", \"H1\", \"H2\"\n",
        "                                    , \"H3\", \"H4\", \"H5\"])\n",
        "\n",
        "preOutput = pd.DataFrame([allOutputActivationsPreLearn[0:9]\n",
        "                         ,allOutputActivationsPreLearn[9:18]\n",
        "                         ,allOutputActivationsPreLearn[18:27]\n",
        "                         ,allOutputActivationsPreLearn[27:36]\n",
        "                         ,allOutputActivationsPreLearn[36:45]\n",
        "                         ,allOutputActivationsPreLearn[45:54]\n",
        "                         ,allOutputActivationsPreLearn[54:63]\n",
        "                         ,allOutputActivationsPreLearn[63:72]\n",
        "                         ,allOutputActivationsPreLearn[72:81]\n",
        "                         ,allOutputActivationsPreLearn[81:90]\n",
        "                         ,allOutputActivationsPreLearn[90:99]\n",
        "                         ,allOutputActivationsPreLearn[99:108]\n",
        "                         ,allOutputActivationsPreLearn[108:117]\n",
        "                         ,allOutputActivationsPreLearn[117:126]\n",
        "                         ,allOutputActivationsPreLearn[126:135]\n",
        "                         ,allOutputActivationsPreLearn[135:144]]\n",
        "                         , index = [\"A\", \"B\", \"C\", \"D\"\n",
        "                                   ,\"E\", \"F\", \"G\", \"H\"\n",
        "                                   ,\"I\", \"J\", \"K\", \"L\"\n",
        "                                   ,\"M\", \"N\", \"O\", \"P\"]\n",
        "                        , columns = [\"o0\", \"o1\", \"o2\"\n",
        "                                    , \"o3\", \"o4\", \"o5\"\n",
        "                                    , \"o6\", \"o7\", \"o8\"])\n",
        "\n",
        "desired = pd.DataFrame([desiredOutput[0:9]\n",
        "                         ,desiredOutput[9:18]\n",
        "                         ,desiredOutput[18:27]\n",
        "                         ,desiredOutput[27:36]\n",
        "                         ,desiredOutput[36:45]\n",
        "                         ,desiredOutput[45:54]\n",
        "                         ,desiredOutput[54:63]\n",
        "                         ,desiredOutput[63:72]\n",
        "                         ,desiredOutput[72:81]\n",
        "                         ,desiredOutput[81:90]\n",
        "                         ,desiredOutput[90:99]\n",
        "                         ,desiredOutput[99:108]\n",
        "                         ,desiredOutput[108:117]\n",
        "                         ,desiredOutput[117:126]\n",
        "                         ,desiredOutput[126:135]\n",
        "                         ,desiredOutput[135:144]]\n",
        "                         , index = [\"A\", \"B\", \"C\", \"D\"\n",
        "                                   ,\"E\", \"F\", \"G\", \"H\"\n",
        "                                   ,\"I\", \"J\", \"K\", \"L\"\n",
        "                                   ,\"M\", \"N\", \"O\", \"P\"]\n",
        "                        , columns = [\"o0\", \"o1\", \"o2\"\n",
        "                                    , \"o3\", \"o4\", \"o5\"\n",
        "                                    , \"o6\", \"o7\", \"o8\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sIkARr96pbhW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create data frames for post-output values of all activations\n",
        "\n",
        "postHidden = pd.DataFrame([allHiddenActivationsPostLearn[0:6]\n",
        "                         ,allHiddenActivationsPostLearn[6:12]\n",
        "                         ,allHiddenActivationsPostLearn[12:18]\n",
        "                         ,allHiddenActivationsPostLearn[18:24]\n",
        "                         ,allHiddenActivationsPostLearn[24:30]\n",
        "                         ,allHiddenActivationsPostLearn[30:36]\n",
        "                         ,allHiddenActivationsPostLearn[36:42]\n",
        "                         ,allHiddenActivationsPostLearn[42:48]\n",
        "                         ,allHiddenActivationsPostLearn[48:54]\n",
        "                         ,allHiddenActivationsPostLearn[54:60]\n",
        "                         ,allHiddenActivationsPostLearn[60:66]\n",
        "                         ,allHiddenActivationsPostLearn[66:72]\n",
        "                         ,allHiddenActivationsPostLearn[72:78]\n",
        "                         ,allHiddenActivationsPostLearn[78:84]\n",
        "                         ,allHiddenActivationsPostLearn[84:90]\n",
        "                         ,allHiddenActivationsPostLearn[90:96]]\n",
        "                         , index = [\"A\", \"B\", \"C\", \"D\"\n",
        "                                   ,\"E\", \"F\", \"G\", \"H\"\n",
        "                                   ,\"I\", \"J\", \"K\", \"L\"\n",
        "                                   ,\"M\", \"N\", \"O\", \"P\"]\n",
        "                        , columns = [\"H0\", \"H1\", \"H2\"\n",
        "                                    , \"H3\", \"H4\", \"H5\"])\n",
        "\n",
        "postOutput = pd.DataFrame([allOutputActivationsPostLearn[0:9]\n",
        "                         ,allOutputActivationsPostLearn[9:18]\n",
        "                         ,allOutputActivationsPostLearn[18:27]\n",
        "                         ,allOutputActivationsPostLearn[27:36]\n",
        "                         ,allOutputActivationsPostLearn[36:45]\n",
        "                         ,allOutputActivationsPostLearn[45:54]\n",
        "                         ,allOutputActivationsPostLearn[54:63]\n",
        "                         ,allOutputActivationsPostLearn[63:72]\n",
        "                         ,allOutputActivationsPostLearn[72:81]\n",
        "                         ,allOutputActivationsPostLearn[81:90]\n",
        "                         ,allOutputActivationsPostLearn[90:99]\n",
        "                         ,allOutputActivationsPostLearn[99:108]\n",
        "                         ,allOutputActivationsPostLearn[108:117]\n",
        "                         ,allOutputActivationsPostLearn[117:126]\n",
        "                         ,allOutputActivationsPostLearn[126:135]\n",
        "                         ,allOutputActivationsPostLearn[135:144]]\n",
        "                         , index = [\"A\", \"B\", \"C\", \"D\"\n",
        "                                   ,\"E\", \"F\", \"G\", \"H\"\n",
        "                                   ,\"I\", \"J\", \"K\", \"L\"\n",
        "                                   ,\"M\", \"N\", \"O\", \"P\"]\n",
        "                        , columns = [\"o0\", \"o1\", \"o2\"\n",
        "                                    , \"o3\", \"o4\", \"o5\"\n",
        "                                    , \"o6\", \"o7\", \"o8\"])\n",
        "\n",
        "postDesired = pd.DataFrame([desiredOutput[0:9]\n",
        "                         ,desiredOutput[9:18]\n",
        "                         ,desiredOutput[18:27]\n",
        "                         ,desiredOutput[27:36]\n",
        "                         ,desiredOutput[36:45]\n",
        "                         ,desiredOutput[45:54]\n",
        "                         ,desiredOutput[54:63]\n",
        "                         ,desiredOutput[63:72]\n",
        "                         ,desiredOutput[72:81]\n",
        "                         ,desiredOutput[81:90]\n",
        "                         ,desiredOutput[90:99]\n",
        "                         ,desiredOutput[99:108]\n",
        "                         ,desiredOutput[108:117]\n",
        "                         ,desiredOutput[117:126]\n",
        "                         ,desiredOutput[126:135]\n",
        "                         ,desiredOutput[135:144]]\n",
        "                         , index = [\"A\", \"B\", \"C\", \"D\"\n",
        "                                   ,\"E\", \"F\", \"G\", \"H\"\n",
        "                                   ,\"I\", \"J\", \"K\", \"L\"\n",
        "                                   ,\"M\", \"N\", \"O\", \"P\"]\n",
        "                        , columns = [\"o0\", \"o1\", \"o2\"\n",
        "                                    , \"o3\", \"o4\", \"o5\"\n",
        "                                    , \"o6\", \"o7\", \"o8\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RHfBPIooGC5p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1906
        },
        "outputId": "e0146f98-623a-4f4a-edf6-be5c1bc35894"
      },
      "cell_type": "code",
      "source": [
        "# Create DataFrames for the final weights going into and out of each hidden node\n",
        "\n",
        "HiddenToOutputFinalWeights = pd.DataFrame([vWeightArrayPost[0]\n",
        "                                          ,vWeightArrayPost[1]\n",
        "                                          ,vWeightArrayPost[2]\n",
        "                                          ,vWeightArrayPost[3]\n",
        "                                          ,vWeightArrayPost[4]\n",
        "                                          ,vWeightArrayPost[5]\n",
        "                                          ,vWeightArrayPost[6]\n",
        "                                          ,vWeightArrayPost[7]\n",
        "                                          ,vWeightArrayPost[8]\n",
        "                                          ], columns = [\"H0\", \"H1\", \"H2\", \"H3\"\n",
        "                                                       ,\"H4\",\"H5\"]\n",
        "                                         , index = [\"o0\", \"o1\", \"o2\", \"o3\", \"o4\"\n",
        "                                                   , \"o5\", \"o6\", \"o7\", \"o8\"])\n",
        "HiddenToOutputFinalWeights = HiddenToOutputFinalWeights.transpose()\n",
        "HiddenToOutputFinalWeights\n",
        "\n",
        "InputToHiddenFinalWeights = pd.DataFrame([wWeightArrayPost[0]\n",
        "                                          ,wWeightArrayPost[1]\n",
        "                                          ,wWeightArrayPost[2]\n",
        "                                          ,wWeightArrayPost[3]\n",
        "                                          ,wWeightArrayPost[4]\n",
        "                                          ,wWeightArrayPost[5]]\n",
        "                                          , index = [\"H0\", \"H1\", \"H2\", \"H3\"\n",
        "                                                       ,\"H4\",\"H5\"])\n",
        "InputToHiddenFinalWeights = InputToHiddenFinalWeights.transpose()\n",
        "\n",
        "InputToHiddenFinalWeights"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>H0</th>\n",
              "      <th>H1</th>\n",
              "      <th>H2</th>\n",
              "      <th>H3</th>\n",
              "      <th>H4</th>\n",
              "      <th>H5</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.434504</td>\n",
              "      <td>-0.488783</td>\n",
              "      <td>0.130245</td>\n",
              "      <td>0.660601</td>\n",
              "      <td>0.308181</td>\n",
              "      <td>0.199522</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.749510</td>\n",
              "      <td>0.084882</td>\n",
              "      <td>0.789451</td>\n",
              "      <td>1.588133</td>\n",
              "      <td>-0.308554</td>\n",
              "      <td>0.000462</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.139928</td>\n",
              "      <td>0.368412</td>\n",
              "      <td>-0.663431</td>\n",
              "      <td>0.226990</td>\n",
              "      <td>-0.265909</td>\n",
              "      <td>1.373826</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.974014</td>\n",
              "      <td>-0.583948</td>\n",
              "      <td>0.741883</td>\n",
              "      <td>-0.267954</td>\n",
              "      <td>-1.537096</td>\n",
              "      <td>1.096118</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.042879</td>\n",
              "      <td>0.154414</td>\n",
              "      <td>0.881985</td>\n",
              "      <td>0.613308</td>\n",
              "      <td>0.094439</td>\n",
              "      <td>0.848292</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1.155759</td>\n",
              "      <td>-0.951931</td>\n",
              "      <td>1.115433</td>\n",
              "      <td>1.081285</td>\n",
              "      <td>-0.874828</td>\n",
              "      <td>0.564274</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.505356</td>\n",
              "      <td>0.136741</td>\n",
              "      <td>1.036721</td>\n",
              "      <td>0.070874</td>\n",
              "      <td>-0.928161</td>\n",
              "      <td>0.500638</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1.213584</td>\n",
              "      <td>0.524270</td>\n",
              "      <td>-0.546613</td>\n",
              "      <td>-0.170270</td>\n",
              "      <td>0.021695</td>\n",
              "      <td>0.721247</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>-1.173564</td>\n",
              "      <td>0.339249</td>\n",
              "      <td>-0.859683</td>\n",
              "      <td>1.063074</td>\n",
              "      <td>-1.042430</td>\n",
              "      <td>0.022618</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>-0.150762</td>\n",
              "      <td>-0.047006</td>\n",
              "      <td>0.808330</td>\n",
              "      <td>0.573355</td>\n",
              "      <td>0.497316</td>\n",
              "      <td>0.215781</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.167154</td>\n",
              "      <td>0.006036</td>\n",
              "      <td>0.766078</td>\n",
              "      <td>-0.774527</td>\n",
              "      <td>0.310309</td>\n",
              "      <td>0.717661</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.180043</td>\n",
              "      <td>-0.559716</td>\n",
              "      <td>0.037415</td>\n",
              "      <td>-0.232565</td>\n",
              "      <td>0.746163</td>\n",
              "      <td>-0.870061</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>-0.694271</td>\n",
              "      <td>-1.243527</td>\n",
              "      <td>-0.630003</td>\n",
              "      <td>-0.708625</td>\n",
              "      <td>-0.653024</td>\n",
              "      <td>-0.121635</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.281620</td>\n",
              "      <td>1.233001</td>\n",
              "      <td>-0.594069</td>\n",
              "      <td>-0.821473</td>\n",
              "      <td>0.220906</td>\n",
              "      <td>0.777587</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>-0.932143</td>\n",
              "      <td>-0.289536</td>\n",
              "      <td>0.450891</td>\n",
              "      <td>1.203512</td>\n",
              "      <td>0.765258</td>\n",
              "      <td>-0.606068</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.235748</td>\n",
              "      <td>-0.415700</td>\n",
              "      <td>-0.997427</td>\n",
              "      <td>-0.277788</td>\n",
              "      <td>-0.040600</td>\n",
              "      <td>-0.751170</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.655208</td>\n",
              "      <td>0.715417</td>\n",
              "      <td>-0.119034</td>\n",
              "      <td>-0.446623</td>\n",
              "      <td>1.272396</td>\n",
              "      <td>0.518244</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>-1.019531</td>\n",
              "      <td>-2.378617</td>\n",
              "      <td>1.802843</td>\n",
              "      <td>-1.772659</td>\n",
              "      <td>-0.316467</td>\n",
              "      <td>0.985079</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.749305</td>\n",
              "      <td>0.421106</td>\n",
              "      <td>0.535920</td>\n",
              "      <td>0.519077</td>\n",
              "      <td>0.987311</td>\n",
              "      <td>-0.042310</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>-0.941119</td>\n",
              "      <td>0.254164</td>\n",
              "      <td>0.374065</td>\n",
              "      <td>0.227912</td>\n",
              "      <td>0.344252</td>\n",
              "      <td>-0.607863</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.093666</td>\n",
              "      <td>-0.316261</td>\n",
              "      <td>-0.135934</td>\n",
              "      <td>-0.614475</td>\n",
              "      <td>0.835061</td>\n",
              "      <td>0.385332</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>-0.479980</td>\n",
              "      <td>-0.620301</td>\n",
              "      <td>-0.935197</td>\n",
              "      <td>0.365424</td>\n",
              "      <td>-0.801617</td>\n",
              "      <td>-0.795194</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>-0.152475</td>\n",
              "      <td>-0.015360</td>\n",
              "      <td>-0.282896</td>\n",
              "      <td>0.302966</td>\n",
              "      <td>0.961536</td>\n",
              "      <td>-0.360577</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0.842410</td>\n",
              "      <td>0.419068</td>\n",
              "      <td>-0.649329</td>\n",
              "      <td>0.291106</td>\n",
              "      <td>0.675934</td>\n",
              "      <td>0.333390</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.839375</td>\n",
              "      <td>-0.826415</td>\n",
              "      <td>0.253794</td>\n",
              "      <td>-0.333662</td>\n",
              "      <td>0.600911</td>\n",
              "      <td>1.046372</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.854434</td>\n",
              "      <td>-0.452231</td>\n",
              "      <td>0.233038</td>\n",
              "      <td>0.305643</td>\n",
              "      <td>0.111555</td>\n",
              "      <td>0.047781</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0.557987</td>\n",
              "      <td>-1.248075</td>\n",
              "      <td>1.369917</td>\n",
              "      <td>-0.348699</td>\n",
              "      <td>-1.192376</td>\n",
              "      <td>0.819762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>0.460134</td>\n",
              "      <td>0.597558</td>\n",
              "      <td>0.349213</td>\n",
              "      <td>0.329658</td>\n",
              "      <td>0.600580</td>\n",
              "      <td>-0.047959</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0.419364</td>\n",
              "      <td>0.439031</td>\n",
              "      <td>-0.176980</td>\n",
              "      <td>-0.324308</td>\n",
              "      <td>-0.087381</td>\n",
              "      <td>0.182621</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>0.445397</td>\n",
              "      <td>-0.427448</td>\n",
              "      <td>0.516395</td>\n",
              "      <td>-0.742909</td>\n",
              "      <td>-0.161144</td>\n",
              "      <td>-0.442624</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51</th>\n",
              "      <td>-0.095972</td>\n",
              "      <td>-0.032877</td>\n",
              "      <td>-0.423947</td>\n",
              "      <td>0.972194</td>\n",
              "      <td>-0.488823</td>\n",
              "      <td>-0.143386</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52</th>\n",
              "      <td>0.446892</td>\n",
              "      <td>-1.507813</td>\n",
              "      <td>-0.645050</td>\n",
              "      <td>1.873589</td>\n",
              "      <td>-2.073565</td>\n",
              "      <td>0.937493</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>-1.294172</td>\n",
              "      <td>-1.205483</td>\n",
              "      <td>0.678547</td>\n",
              "      <td>-0.750763</td>\n",
              "      <td>-0.416279</td>\n",
              "      <td>0.997814</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54</th>\n",
              "      <td>-1.151571</td>\n",
              "      <td>0.137427</td>\n",
              "      <td>0.566030</td>\n",
              "      <td>-0.732969</td>\n",
              "      <td>-0.178841</td>\n",
              "      <td>0.746598</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55</th>\n",
              "      <td>-0.338517</td>\n",
              "      <td>-0.397203</td>\n",
              "      <td>-0.109926</td>\n",
              "      <td>0.316335</td>\n",
              "      <td>-0.958326</td>\n",
              "      <td>-0.578976</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56</th>\n",
              "      <td>0.754814</td>\n",
              "      <td>0.428090</td>\n",
              "      <td>-0.240482</td>\n",
              "      <td>0.250419</td>\n",
              "      <td>-0.162731</td>\n",
              "      <td>0.696982</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57</th>\n",
              "      <td>-1.079742</td>\n",
              "      <td>-0.963979</td>\n",
              "      <td>-0.062609</td>\n",
              "      <td>0.484420</td>\n",
              "      <td>-0.294141</td>\n",
              "      <td>-0.204385</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>58</th>\n",
              "      <td>0.735044</td>\n",
              "      <td>0.062679</td>\n",
              "      <td>0.211430</td>\n",
              "      <td>0.886332</td>\n",
              "      <td>-0.871089</td>\n",
              "      <td>-0.012380</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59</th>\n",
              "      <td>-0.548946</td>\n",
              "      <td>-0.446550</td>\n",
              "      <td>-0.308555</td>\n",
              "      <td>0.128953</td>\n",
              "      <td>-0.459535</td>\n",
              "      <td>-0.076725</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60</th>\n",
              "      <td>-0.640384</td>\n",
              "      <td>0.401236</td>\n",
              "      <td>0.863009</td>\n",
              "      <td>0.381935</td>\n",
              "      <td>-0.569968</td>\n",
              "      <td>0.599959</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>61</th>\n",
              "      <td>0.361599</td>\n",
              "      <td>0.946000</td>\n",
              "      <td>0.862303</td>\n",
              "      <td>-0.381916</td>\n",
              "      <td>-0.700446</td>\n",
              "      <td>0.694223</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>62</th>\n",
              "      <td>-0.326819</td>\n",
              "      <td>-0.558281</td>\n",
              "      <td>-0.801178</td>\n",
              "      <td>0.815710</td>\n",
              "      <td>0.113441</td>\n",
              "      <td>0.073539</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>63</th>\n",
              "      <td>0.332621</td>\n",
              "      <td>0.081485</td>\n",
              "      <td>0.158249</td>\n",
              "      <td>-0.020840</td>\n",
              "      <td>0.460466</td>\n",
              "      <td>-0.594142</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>64</th>\n",
              "      <td>0.148344</td>\n",
              "      <td>0.333924</td>\n",
              "      <td>0.357134</td>\n",
              "      <td>-0.133556</td>\n",
              "      <td>-0.677442</td>\n",
              "      <td>-0.282914</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65</th>\n",
              "      <td>0.669950</td>\n",
              "      <td>1.192698</td>\n",
              "      <td>0.742461</td>\n",
              "      <td>-0.333723</td>\n",
              "      <td>-0.921895</td>\n",
              "      <td>0.766686</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66</th>\n",
              "      <td>-0.479485</td>\n",
              "      <td>-0.486320</td>\n",
              "      <td>0.079525</td>\n",
              "      <td>-0.117038</td>\n",
              "      <td>0.074168</td>\n",
              "      <td>0.173929</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>67</th>\n",
              "      <td>-0.222652</td>\n",
              "      <td>0.799349</td>\n",
              "      <td>0.972326</td>\n",
              "      <td>0.382260</td>\n",
              "      <td>-0.063726</td>\n",
              "      <td>0.198309</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68</th>\n",
              "      <td>-0.755817</td>\n",
              "      <td>-0.162976</td>\n",
              "      <td>-0.220533</td>\n",
              "      <td>-0.269564</td>\n",
              "      <td>0.682552</td>\n",
              "      <td>0.414198</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69</th>\n",
              "      <td>-0.281772</td>\n",
              "      <td>0.715849</td>\n",
              "      <td>0.083670</td>\n",
              "      <td>-1.193940</td>\n",
              "      <td>0.196216</td>\n",
              "      <td>0.151308</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>70</th>\n",
              "      <td>-0.132035</td>\n",
              "      <td>0.018131</td>\n",
              "      <td>0.176987</td>\n",
              "      <td>-0.418749</td>\n",
              "      <td>0.239076</td>\n",
              "      <td>0.604415</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71</th>\n",
              "      <td>-0.526967</td>\n",
              "      <td>-2.054435</td>\n",
              "      <td>-0.138487</td>\n",
              "      <td>-0.640927</td>\n",
              "      <td>0.379745</td>\n",
              "      <td>-0.542073</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>72</th>\n",
              "      <td>0.583725</td>\n",
              "      <td>-0.753574</td>\n",
              "      <td>-1.182256</td>\n",
              "      <td>0.039511</td>\n",
              "      <td>0.360337</td>\n",
              "      <td>0.306739</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73</th>\n",
              "      <td>0.621524</td>\n",
              "      <td>0.223446</td>\n",
              "      <td>0.057783</td>\n",
              "      <td>0.350637</td>\n",
              "      <td>0.776465</td>\n",
              "      <td>-0.262974</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>74</th>\n",
              "      <td>0.260777</td>\n",
              "      <td>0.689511</td>\n",
              "      <td>-0.858864</td>\n",
              "      <td>-0.105743</td>\n",
              "      <td>0.035360</td>\n",
              "      <td>-0.798797</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75</th>\n",
              "      <td>0.546500</td>\n",
              "      <td>0.699077</td>\n",
              "      <td>0.147395</td>\n",
              "      <td>-1.473438</td>\n",
              "      <td>-0.533398</td>\n",
              "      <td>-0.379967</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76</th>\n",
              "      <td>-0.568421</td>\n",
              "      <td>0.023650</td>\n",
              "      <td>0.119810</td>\n",
              "      <td>-0.726894</td>\n",
              "      <td>-0.501216</td>\n",
              "      <td>-0.115364</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>77</th>\n",
              "      <td>1.179620</td>\n",
              "      <td>0.507862</td>\n",
              "      <td>0.160949</td>\n",
              "      <td>-0.840718</td>\n",
              "      <td>-0.673402</td>\n",
              "      <td>0.173014</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>78</th>\n",
              "      <td>-0.024664</td>\n",
              "      <td>-0.032842</td>\n",
              "      <td>-0.873934</td>\n",
              "      <td>-0.118272</td>\n",
              "      <td>0.371869</td>\n",
              "      <td>-1.120477</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79</th>\n",
              "      <td>0.403405</td>\n",
              "      <td>0.725065</td>\n",
              "      <td>-0.820760</td>\n",
              "      <td>0.180681</td>\n",
              "      <td>1.077428</td>\n",
              "      <td>0.046858</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80</th>\n",
              "      <td>0.237640</td>\n",
              "      <td>0.456697</td>\n",
              "      <td>-0.651158</td>\n",
              "      <td>-1.286736</td>\n",
              "      <td>-0.163607</td>\n",
              "      <td>0.626159</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>81 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          H0        H1        H2        H3        H4        H5\n",
              "0   0.434504 -0.488783  0.130245  0.660601  0.308181  0.199522\n",
              "1   0.749510  0.084882  0.789451  1.588133 -0.308554  0.000462\n",
              "2   1.139928  0.368412 -0.663431  0.226990 -0.265909  1.373826\n",
              "3   0.974014 -0.583948  0.741883 -0.267954 -1.537096  1.096118\n",
              "4   0.042879  0.154414  0.881985  0.613308  0.094439  0.848292\n",
              "5   1.155759 -0.951931  1.115433  1.081285 -0.874828  0.564274\n",
              "6   0.505356  0.136741  1.036721  0.070874 -0.928161  0.500638\n",
              "7   1.213584  0.524270 -0.546613 -0.170270  0.021695  0.721247\n",
              "8  -1.173564  0.339249 -0.859683  1.063074 -1.042430  0.022618\n",
              "9  -0.150762 -0.047006  0.808330  0.573355  0.497316  0.215781\n",
              "10  0.167154  0.006036  0.766078 -0.774527  0.310309  0.717661\n",
              "11  0.180043 -0.559716  0.037415 -0.232565  0.746163 -0.870061\n",
              "12 -0.694271 -1.243527 -0.630003 -0.708625 -0.653024 -0.121635\n",
              "13  0.281620  1.233001 -0.594069 -0.821473  0.220906  0.777587\n",
              "14 -0.932143 -0.289536  0.450891  1.203512  0.765258 -0.606068\n",
              "15  0.235748 -0.415700 -0.997427 -0.277788 -0.040600 -0.751170\n",
              "16  0.655208  0.715417 -0.119034 -0.446623  1.272396  0.518244\n",
              "17 -1.019531 -2.378617  1.802843 -1.772659 -0.316467  0.985079\n",
              "18  0.749305  0.421106  0.535920  0.519077  0.987311 -0.042310\n",
              "19 -0.941119  0.254164  0.374065  0.227912  0.344252 -0.607863\n",
              "20  0.093666 -0.316261 -0.135934 -0.614475  0.835061  0.385332\n",
              "21 -0.479980 -0.620301 -0.935197  0.365424 -0.801617 -0.795194\n",
              "22 -0.152475 -0.015360 -0.282896  0.302966  0.961536 -0.360577\n",
              "23  0.842410  0.419068 -0.649329  0.291106  0.675934  0.333390\n",
              "24  0.839375 -0.826415  0.253794 -0.333662  0.600911  1.046372\n",
              "25  0.854434 -0.452231  0.233038  0.305643  0.111555  0.047781\n",
              "26  0.557987 -1.248075  1.369917 -0.348699 -1.192376  0.819762\n",
              "27  0.460134  0.597558  0.349213  0.329658  0.600580 -0.047959\n",
              "28  0.419364  0.439031 -0.176980 -0.324308 -0.087381  0.182621\n",
              "29  0.445397 -0.427448  0.516395 -0.742909 -0.161144 -0.442624\n",
              "..       ...       ...       ...       ...       ...       ...\n",
              "51 -0.095972 -0.032877 -0.423947  0.972194 -0.488823 -0.143386\n",
              "52  0.446892 -1.507813 -0.645050  1.873589 -2.073565  0.937493\n",
              "53 -1.294172 -1.205483  0.678547 -0.750763 -0.416279  0.997814\n",
              "54 -1.151571  0.137427  0.566030 -0.732969 -0.178841  0.746598\n",
              "55 -0.338517 -0.397203 -0.109926  0.316335 -0.958326 -0.578976\n",
              "56  0.754814  0.428090 -0.240482  0.250419 -0.162731  0.696982\n",
              "57 -1.079742 -0.963979 -0.062609  0.484420 -0.294141 -0.204385\n",
              "58  0.735044  0.062679  0.211430  0.886332 -0.871089 -0.012380\n",
              "59 -0.548946 -0.446550 -0.308555  0.128953 -0.459535 -0.076725\n",
              "60 -0.640384  0.401236  0.863009  0.381935 -0.569968  0.599959\n",
              "61  0.361599  0.946000  0.862303 -0.381916 -0.700446  0.694223\n",
              "62 -0.326819 -0.558281 -0.801178  0.815710  0.113441  0.073539\n",
              "63  0.332621  0.081485  0.158249 -0.020840  0.460466 -0.594142\n",
              "64  0.148344  0.333924  0.357134 -0.133556 -0.677442 -0.282914\n",
              "65  0.669950  1.192698  0.742461 -0.333723 -0.921895  0.766686\n",
              "66 -0.479485 -0.486320  0.079525 -0.117038  0.074168  0.173929\n",
              "67 -0.222652  0.799349  0.972326  0.382260 -0.063726  0.198309\n",
              "68 -0.755817 -0.162976 -0.220533 -0.269564  0.682552  0.414198\n",
              "69 -0.281772  0.715849  0.083670 -1.193940  0.196216  0.151308\n",
              "70 -0.132035  0.018131  0.176987 -0.418749  0.239076  0.604415\n",
              "71 -0.526967 -2.054435 -0.138487 -0.640927  0.379745 -0.542073\n",
              "72  0.583725 -0.753574 -1.182256  0.039511  0.360337  0.306739\n",
              "73  0.621524  0.223446  0.057783  0.350637  0.776465 -0.262974\n",
              "74  0.260777  0.689511 -0.858864 -0.105743  0.035360 -0.798797\n",
              "75  0.546500  0.699077  0.147395 -1.473438 -0.533398 -0.379967\n",
              "76 -0.568421  0.023650  0.119810 -0.726894 -0.501216 -0.115364\n",
              "77  1.179620  0.507862  0.160949 -0.840718 -0.673402  0.173014\n",
              "78 -0.024664 -0.032842 -0.873934 -0.118272  0.371869 -1.120477\n",
              "79  0.403405  0.725065 -0.820760  0.180681  1.077428  0.046858\n",
              "80  0.237640  0.456697 -0.651158 -1.286736 -0.163607  0.626159\n",
              "\n",
              "[81 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "metadata": {
        "id": "R1BDnz6jFWfT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Results"
      ]
    },
    {
      "metadata": {
        "id": "rB9jEjxyFrBt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "qHFODzRjz6Qh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "outputId": "dc324485-3188-4fd9-e805-aea262fc861f"
      },
      "cell_type": "code",
      "source": [
        "postOutput"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>o0</th>\n",
              "      <th>o1</th>\n",
              "      <th>o2</th>\n",
              "      <th>o3</th>\n",
              "      <th>o4</th>\n",
              "      <th>o5</th>\n",
              "      <th>o6</th>\n",
              "      <th>o7</th>\n",
              "      <th>o8</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>A</th>\n",
              "      <td>0.911144</td>\n",
              "      <td>0.021944</td>\n",
              "      <td>0.030829</td>\n",
              "      <td>0.005760</td>\n",
              "      <td>0.051533</td>\n",
              "      <td>0.002695</td>\n",
              "      <td>0.030430</td>\n",
              "      <td>0.009168</td>\n",
              "      <td>0.062455</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>B</th>\n",
              "      <td>0.012385</td>\n",
              "      <td>0.948236</td>\n",
              "      <td>0.049284</td>\n",
              "      <td>0.080216</td>\n",
              "      <td>0.041040</td>\n",
              "      <td>0.038110</td>\n",
              "      <td>0.033892</td>\n",
              "      <td>0.000974</td>\n",
              "      <td>0.001448</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>C</th>\n",
              "      <td>0.000503</td>\n",
              "      <td>0.012657</td>\n",
              "      <td>0.029729</td>\n",
              "      <td>0.024473</td>\n",
              "      <td>0.041281</td>\n",
              "      <td>0.061576</td>\n",
              "      <td>0.006437</td>\n",
              "      <td>0.021542</td>\n",
              "      <td>0.005686</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>D</th>\n",
              "      <td>0.002451</td>\n",
              "      <td>0.027884</td>\n",
              "      <td>0.028834</td>\n",
              "      <td>0.864660</td>\n",
              "      <td>0.000104</td>\n",
              "      <td>0.018847</td>\n",
              "      <td>0.005616</td>\n",
              "      <td>0.014634</td>\n",
              "      <td>0.071760</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>E</th>\n",
              "      <td>0.040729</td>\n",
              "      <td>0.028190</td>\n",
              "      <td>0.023197</td>\n",
              "      <td>0.001355</td>\n",
              "      <td>0.926751</td>\n",
              "      <td>0.019887</td>\n",
              "      <td>0.008058</td>\n",
              "      <td>0.018787</td>\n",
              "      <td>0.000066</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>F</th>\n",
              "      <td>0.039969</td>\n",
              "      <td>0.052348</td>\n",
              "      <td>0.026218</td>\n",
              "      <td>0.001354</td>\n",
              "      <td>0.915299</td>\n",
              "      <td>0.027022</td>\n",
              "      <td>0.008540</td>\n",
              "      <td>0.014014</td>\n",
              "      <td>0.000084</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>G</th>\n",
              "      <td>0.009253</td>\n",
              "      <td>0.915426</td>\n",
              "      <td>0.047978</td>\n",
              "      <td>0.081994</td>\n",
              "      <td>0.035819</td>\n",
              "      <td>0.039021</td>\n",
              "      <td>0.033661</td>\n",
              "      <td>0.001339</td>\n",
              "      <td>0.001793</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>H</th>\n",
              "      <td>0.907431</td>\n",
              "      <td>0.032450</td>\n",
              "      <td>0.033022</td>\n",
              "      <td>0.006322</td>\n",
              "      <td>0.046313</td>\n",
              "      <td>0.003268</td>\n",
              "      <td>0.031427</td>\n",
              "      <td>0.007959</td>\n",
              "      <td>0.067259</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>I</th>\n",
              "      <td>0.000520</td>\n",
              "      <td>0.026055</td>\n",
              "      <td>0.073520</td>\n",
              "      <td>0.066481</td>\n",
              "      <td>0.028787</td>\n",
              "      <td>0.916739</td>\n",
              "      <td>0.000463</td>\n",
              "      <td>0.046076</td>\n",
              "      <td>0.013571</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>J</th>\n",
              "      <td>0.000621</td>\n",
              "      <td>0.023020</td>\n",
              "      <td>0.074315</td>\n",
              "      <td>0.064233</td>\n",
              "      <td>0.031507</td>\n",
              "      <td>0.923106</td>\n",
              "      <td>0.000434</td>\n",
              "      <td>0.051699</td>\n",
              "      <td>0.013557</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>K</th>\n",
              "      <td>0.030152</td>\n",
              "      <td>0.061164</td>\n",
              "      <td>0.025107</td>\n",
              "      <td>0.013780</td>\n",
              "      <td>0.023318</td>\n",
              "      <td>0.000287</td>\n",
              "      <td>0.894896</td>\n",
              "      <td>0.075383</td>\n",
              "      <td>0.056907</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>L</th>\n",
              "      <td>0.002102</td>\n",
              "      <td>0.000178</td>\n",
              "      <td>0.027983</td>\n",
              "      <td>0.075977</td>\n",
              "      <td>0.066344</td>\n",
              "      <td>0.050165</td>\n",
              "      <td>0.073428</td>\n",
              "      <td>0.902528</td>\n",
              "      <td>0.010072</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>M</th>\n",
              "      <td>0.048751</td>\n",
              "      <td>0.007691</td>\n",
              "      <td>0.042174</td>\n",
              "      <td>0.115644</td>\n",
              "      <td>0.000108</td>\n",
              "      <td>0.010187</td>\n",
              "      <td>0.043351</td>\n",
              "      <td>0.018700</td>\n",
              "      <td>0.921713</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>N</th>\n",
              "      <td>0.075235</td>\n",
              "      <td>0.009630</td>\n",
              "      <td>0.040709</td>\n",
              "      <td>0.116035</td>\n",
              "      <td>0.000101</td>\n",
              "      <td>0.012898</td>\n",
              "      <td>0.011613</td>\n",
              "      <td>0.006230</td>\n",
              "      <td>0.895899</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>O</th>\n",
              "      <td>0.002410</td>\n",
              "      <td>0.030153</td>\n",
              "      <td>0.030066</td>\n",
              "      <td>0.863994</td>\n",
              "      <td>0.000106</td>\n",
              "      <td>0.022837</td>\n",
              "      <td>0.005325</td>\n",
              "      <td>0.014860</td>\n",
              "      <td>0.074141</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>P</th>\n",
              "      <td>0.015697</td>\n",
              "      <td>0.889407</td>\n",
              "      <td>0.042533</td>\n",
              "      <td>0.056814</td>\n",
              "      <td>0.043582</td>\n",
              "      <td>0.020611</td>\n",
              "      <td>0.036400</td>\n",
              "      <td>0.001086</td>\n",
              "      <td>0.001757</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         o0        o1        o2        o3        o4        o5        o6  \\\n",
              "A  0.911144  0.021944  0.030829  0.005760  0.051533  0.002695  0.030430   \n",
              "B  0.012385  0.948236  0.049284  0.080216  0.041040  0.038110  0.033892   \n",
              "C  0.000503  0.012657  0.029729  0.024473  0.041281  0.061576  0.006437   \n",
              "D  0.002451  0.027884  0.028834  0.864660  0.000104  0.018847  0.005616   \n",
              "E  0.040729  0.028190  0.023197  0.001355  0.926751  0.019887  0.008058   \n",
              "F  0.039969  0.052348  0.026218  0.001354  0.915299  0.027022  0.008540   \n",
              "G  0.009253  0.915426  0.047978  0.081994  0.035819  0.039021  0.033661   \n",
              "H  0.907431  0.032450  0.033022  0.006322  0.046313  0.003268  0.031427   \n",
              "I  0.000520  0.026055  0.073520  0.066481  0.028787  0.916739  0.000463   \n",
              "J  0.000621  0.023020  0.074315  0.064233  0.031507  0.923106  0.000434   \n",
              "K  0.030152  0.061164  0.025107  0.013780  0.023318  0.000287  0.894896   \n",
              "L  0.002102  0.000178  0.027983  0.075977  0.066344  0.050165  0.073428   \n",
              "M  0.048751  0.007691  0.042174  0.115644  0.000108  0.010187  0.043351   \n",
              "N  0.075235  0.009630  0.040709  0.116035  0.000101  0.012898  0.011613   \n",
              "O  0.002410  0.030153  0.030066  0.863994  0.000106  0.022837  0.005325   \n",
              "P  0.015697  0.889407  0.042533  0.056814  0.043582  0.020611  0.036400   \n",
              "\n",
              "         o7        o8  \n",
              "A  0.009168  0.062455  \n",
              "B  0.000974  0.001448  \n",
              "C  0.021542  0.005686  \n",
              "D  0.014634  0.071760  \n",
              "E  0.018787  0.000066  \n",
              "F  0.014014  0.000084  \n",
              "G  0.001339  0.001793  \n",
              "H  0.007959  0.067259  \n",
              "I  0.046076  0.013571  \n",
              "J  0.051699  0.013557  \n",
              "K  0.075383  0.056907  \n",
              "L  0.902528  0.010072  \n",
              "M  0.018700  0.921713  \n",
              "N  0.006230  0.895899  \n",
              "O  0.014860  0.074141  \n",
              "P  0.001086  0.001757  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "metadata": {
        "id": "662vkGIJ2kZc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}