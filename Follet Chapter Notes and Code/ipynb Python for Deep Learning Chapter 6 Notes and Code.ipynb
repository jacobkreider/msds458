{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Deep Learning with Python* Chapter 6: Deep Learning for Texts and Sequences\n",
    "##### *Notebook by Jacob Kreider*\n",
    "\n",
    "*The below code is pulled directly from Deep Learning with Python by Francois Chollet (Manning Publications, 2018). Code comments, notes, and commentary are a mix of mine and the book author's. I will put anything directly from the book in quotes, with the exception of code comments. They might be mine, might be his.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This chapter covers both RNNs (recurrent neural networks) and 1D convnets (CNNs, hereafter) for text processing and timeseries prediction/classification. \n",
    "\n",
    "\"\n",
    "Applications of these algorithms include the following:\n",
    "\n",
    "* Document classification and timeseries classification, such as identifying the topic of an article or the author of a book\n",
    "* Timeseries comparisons, such as estimating how closely related two documents or two stock tickers are\n",
    "* Sequence-to-sequence learning, such as decoding an English sentence into French\n",
    "* Sentiment analysis, such as classifying the sentiment of tweets or movie reviews as positive or negative\n",
    "* Timeseries forecasting, such as predicting the future weather at a certain location, given recent weather data\"\n",
    "\n",
    "Example networks in this chapter are: sentiment analysis using the IMDb dataset and temperature forecasting.\n",
    "\n",
    "### 6.1 Working with text data\n",
    "\n",
    "The methods we use here are for very basic NLP-- good enough for doc classifiers, author identifications, etc. This doesn't give a deep understanding of text, but does \"map the statistical structureof written language\" which is enough for our examples here.\n",
    "\n",
    "As with images, we need to convert the raw text into numeric tensors, by *vectorizing* the text. this can be done by:\n",
    "* Segmenting the text into words, then transforming each word into a vector\n",
    "* Segmenting the text into characters, then transforming each character in a vector; or,\n",
    "* Extract *n-grams* (overlapping groups of multiple consecutive words/characters) and transforming each n-gram into a character\n",
    "\n",
    "***Tokens***: the different units that text can be broken into (i.e words, characters, or n-grams)\n",
    "\n",
    "***Tokenization***: the process of creating these tokens from the text\n",
    "\n",
    "How to associate vectors with tokens: one-hot encoding and token embedding\n",
    "\n",
    "*See chapter section for a detailed explanation of how n-gram tokenization works.* But, one thing to remember is that bag-of-words models (the term for using n-gram tokenization) doesn't preserve word order overall. It just creates a set of the n-gram combinations. As such, it is better for shallow language processing than deep learning. Bag-of-words is a step of *feature engineering*, which deep learning tries to minimize and handle on its own.\n",
    "\n",
    "#### 6.1.1 One-hot encoding of words and characters\n",
    "\n",
    "Most common way to turn a token into a vector. Associates a unique integer index with every word, then creating a binary vector with size N = to the size of the vocabulary with all zeros except for a 1 in the *i*th entry.\n",
    "\n",
    "Let's give it a shot:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 6.1 Word-level one-hot encoding (toy example)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
    "\n",
    "tokenIndex = {} # Empty index for all the tokens in samples\n",
    "for sample in samples:\n",
    "    for word in sample.split(): # splits apart all the words. Could also remove puncuation/common words\n",
    "        if word not in tokenIndex:\n",
    "            tokenIndex[word] = len(tokenIndex) + 1 # assigns an index at each word, ignores index[0]\n",
    "            \n",
    "maxLength = 10 # This limits the length of the vector. So, here, only first 10 words of each sample are used\n",
    "\n",
    "# Create a place to store the results\n",
    "results = np.zeros(shape = (len(samples)\n",
    "                            , maxLength\n",
    "                           , max(tokenIndex.values()) + 1))\n",
    "\n",
    "# one-hot encode the tokenIndex\n",
    "\n",
    "for i, sample in enumerate(samples):\n",
    "    for j, word in list(enumerate(sample.split()))[:maxLength]:\n",
    "        index = tokenIndex.get(word)\n",
    "        results[i, j, index] = 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 6.2 Character-level one-hot encoding (toy example)\n",
    "\n",
    "import string\n",
    "\n",
    "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
    "\n",
    "characters = string.printable\n",
    "tokenIndex = dict(zip(range(1, len(characters) + 1), characters))\n",
    "\n",
    "maxLength = 50\n",
    "\n",
    "results = np.zeros((len(samples), maxLength, max(tokenIndex.keys()) + 1))\n",
    "\n",
    "for i, sample in enumerate(samples):\n",
    "    for j, character in enumerate(sample):\n",
    "        index = tokenIndex.get(character)\n",
    "        results[i, j, index] = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use keras' built-in one-hot encoding. It strips out special characters and only considers the N most common words in the data (a commonly used restriction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "# Listing 6.3 Using Keras for word-level one-hot encoding\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
    "\n",
    "tokenizer = Tokenizer(num_words = 1000)\n",
    "tokenizer.fit_on_texts(samples)\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(samples)\n",
    "\n",
    "oneHotResults = tokenizer.texts_to_matrix(samples, mode = 'binary')\n",
    "\n",
    "wordIndex = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(wordIndex))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For data where the number of unique tokens is extremely large, we can use *one-hot hashing trick* which hashes words into vectors of fixed size, rather than assigning an index to each.\n",
    "\n",
    "To avoid having multiple words assigned to the same has (called *hash collisions*), the dimensionality of the hashing space should be much larger than the total number of unique tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 6.4 Word-level one-hot encoding with hashing trick\n",
    "\n",
    "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
    "\n",
    "dimensionality = 1000 # if we had close to 1K words, there'd be a ton of hash collisions\n",
    "maxLength = 10\n",
    "\n",
    "results = np.zeros((len(samples), maxLength, dimensionality))\n",
    "\n",
    "for i, sample in enumerate(samples):\n",
    "    for j, word in list(enumerate(sample.split()))[:maxLength]:\n",
    "        index = abs(hash(word)) % dimensionality # hashes the word into random in b/w 1 and 1000\n",
    "        results[i, j, index] = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1.2 Using word embeddings\n",
    "\n",
    "One-hot encoding creates *binary, sparse, and high dimensional* vectors, word embeddings are low-dimensional floating-point vectors (i.e. dense vectors). In essence, word embeddings \"pack more information into far fewer dimensions.\"\n",
    "\n",
    "Two ways to obtain:\n",
    "\n",
    "1) \"Learn word embeddings jointly with the main task you care about (such as document classification or sentiment prediction). In this setup, you start with random word vectors and then learn word vectors in the same way you learn the weights of a neural network.\"\n",
    "\n",
    "2) \"Load into your model word embeddings that were precomputed using a different machine-learning task than the one youâ€™re trying to solve. These are called *pretrained word embeddings*.\"\n",
    "\n",
    "###### Learning word embeddings with the Embedding Layer\n",
    "\n",
    "The geometric relationships between words should refelct the semantic relationships. For example, \"accurate\" and \"exact\" should be similarly embedded. So, we should account for both geometric distance (as in *exact* and *precise*), as well as direction (as in *cat* to *tiger*).\n",
    "\n",
    "When done correctly, we should be able to make meaningful transformations (e.g. adding *female* to *king* == *queen*). This is obviously very task dependent. So, it makes sense to learn a new embedding space with each new task, and this can be done relatively easily, thanks to our old friend backpropagation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List 6.5 Instantiating an Embedding Layer with keras\n",
    "\n",
    "from keras.layers import Embedding\n",
    "\n",
    "embeddingLayer = Embedding(1000, 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can think of the embedding layer as \"a dictionary that maps integer indices (which stand for specific words) to dense vectors...it is effectively a dictionary lookup.\"\n",
    "\n",
    "Word Index ------> Embedding Layer -----> Corresponding word vector\n",
    "\n",
    "The Embedding Layer takes a 2D tensor of integers as input (shape = (samples, sequenceLength)). Because they get packed into a single tensor, shorter sequences should be padded with zeros and longer ones should be truncated (if need be, depending on batch size).\n",
    "\n",
    "The Embedding Layer returns a 3D floating-point tensor of shape(samples, sequenceLength, embeddingDimensionality), which can then be processed by RNNs and 1D CNNs.\n",
    "\n",
    "When an Embedding Layer is instantiated, as above, it starts with random weights, and then they are adjusted via backprop, just like a normal network.\n",
    "\n",
    "Now, we'll apply this to the IMDb data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 6.6 Loading the IMDb data for use with Embedding Layer\n",
    "\n",
    "from keras.datasets import imdb\n",
    "from keras import preprocessing\n",
    "\n",
    "# Set the number of words to consider as features\n",
    "maxFeatures = 10000\n",
    "# Cuts off the text after this number of words\n",
    "maxlen = 20\n",
    "\n",
    "(xTrain, yTrain), (xTest, yTest) = imdb.load_data(\n",
    "    num_words = maxFeatures)\n",
    "\n",
    "# Turn the list of integers into a 2D integer tensor, shape(samples, maxlen)\n",
    "xTrain = preprocessing.sequence.pad_sequences(xTrain, maxlen = maxlen)\n",
    "xTest = preprocessing.sequence.pad_sequences(xTest, maxlen = maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 20, 8)             80000     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 161       \n",
      "=================================================================\n",
      "Total params: 80,161\n",
      "Trainable params: 80,161\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "20000/20000 [==============================] - 2s 103us/step - loss: 0.6759 - acc: 0.6051 - val_loss: 0.6399 - val_acc: 0.6806\n",
      "Epoch 2/10\n",
      "20000/20000 [==============================] - 2s 91us/step - loss: 0.5660 - acc: 0.7429 - val_loss: 0.5470 - val_acc: 0.7194\n",
      "Epoch 3/10\n",
      "20000/20000 [==============================] - 2s 78us/step - loss: 0.4754 - acc: 0.7808 - val_loss: 0.5115 - val_acc: 0.7382\n",
      "Epoch 4/10\n",
      "20000/20000 [==============================] - 2s 84us/step - loss: 0.4263 - acc: 0.8078 - val_loss: 0.5009 - val_acc: 0.7442\n",
      "Epoch 5/10\n",
      "20000/20000 [==============================] - 2s 87us/step - loss: 0.3929 - acc: 0.8261 - val_loss: 0.4982 - val_acc: 0.7534\n",
      "Epoch 6/10\n",
      "20000/20000 [==============================] - 2s 76us/step - loss: 0.3665 - acc: 0.8400 - val_loss: 0.5015 - val_acc: 0.7528\n",
      "Epoch 7/10\n",
      "20000/20000 [==============================] - 2s 82us/step - loss: 0.3431 - acc: 0.8540 - val_loss: 0.5054 - val_acc: 0.7526\n",
      "Epoch 8/10\n",
      "20000/20000 [==============================] - 2s 79us/step - loss: 0.3218 - acc: 0.8657 - val_loss: 0.5134 - val_acc: 0.7476\n",
      "Epoch 9/10\n",
      "20000/20000 [==============================] - 2s 85us/step - loss: 0.3017 - acc: 0.8770 - val_loss: 0.5217 - val_acc: 0.7486\n",
      "Epoch 10/10\n",
      "20000/20000 [==============================] - 2s 82us/step - loss: 0.2833 - acc: 0.8860 - val_loss: 0.5307 - val_acc: 0.7466\n"
     ]
    }
   ],
   "source": [
    "# Listing 6.7 Using an Embedding LAyer and classifier on the IMDb data\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense\n",
    "\n",
    "model = Sequential()\n",
    "# Sepcify the max inout length to the Embed layer,so we can flatten the inputs later\n",
    "# Activations after this layer wil have shae(samples, maxlen, 8)\n",
    "model.add(Embedding(10000, 8, input_length = maxlen))\n",
    "\n",
    "# Flatten the 3D tensor output from the Embedding layer into\n",
    "# a 2D tensor of shape (samples, maxlen * 8)\n",
    "model.add(Flatten())\n",
    "\n",
    "# Now, we'll add the classifier\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "model.compile(optimizer = 'rmsprop', loss = 'binary_crossentropy', metrics = ['acc'])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(xTrain, yTrain\n",
    "                   , epochs = 10\n",
    "                   , batch_size = 32\n",
    "                   , validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The really basic model above gave us ~75% accuracy. Pretty good, considering we were only looking at the first 20 words in each review.\n",
    "\n",
    "What it *didn't* do was account for any inter-word relationships. It saw all words as standalone. Adding recurrent or 1D convolutional layers on top will help learn features that account for the meanings of sequences of words.\n",
    "\n",
    "###### Using pretrained word embeddings\n",
    "\n",
    "IF you don't have enough data to learn the embedding space on your own, you can download highly structured, pretrained word embeddings. Some example of these pretraining embedding spaces are Word2Vec and GloVe.\n",
    "\n",
    "#### 6.1.3 Putting it all together: from raw text to word embeddings\n",
    "\n",
    "Start this section by downloading and installing the raw IMDb dataset from [here](http://mng.bz/0tIo) . Change path in Listing 6.8 to local directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 6.8 Processing the labels of the raw IMDb data\n",
    "\n",
    "import os\n",
    "imdbDir = '/home/jacob/MSDS-git/msds458/Data/aclImdb'\n",
    "trainDir = os.path.join(imdbDir, 'train')\n",
    "\n",
    "labels = []\n",
    "texts = []\n",
    "\n",
    "for label_type in ['neg', 'pos']:\n",
    "    dirName = os.path.join(trainDir, label_type)\n",
    "    for fname in os.listdir(dirName):\n",
    "        if fname[-4:] == '.txt':\n",
    "            f = open(os.path.join(dirName, fname))\n",
    "            texts.append(f.read())\n",
    "            f.close()\n",
    "            if label_type == 'neg':\n",
    "                labels.append(0)\n",
    "            else:\n",
    "                labels.append(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
